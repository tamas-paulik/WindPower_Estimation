{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6FIi8t8NUTEJ"
   },
   "source": [
    "# Time Series Prediction with LSTM Using PyTorch\n",
    "\n",
    "This kernel is based on *datasets* from\n",
    "\n",
    "[Time Series Forecasting with the Long Short-Term Memory Network in Python](https://machinelearningmastery.com/time-series-forecasting-long-short-term-memory-network-python/)\n",
    "\n",
    "[Time Series Prediction with LSTM Recurrent Neural Networks in Python with Keras](https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9DqRvEBU4aL"
   },
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B5JAorx0dJ5i",
    "outputId": "ae778a19-cbe5-44cd-9735-53caed8543b0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "id": "lJfI5TLedR6q",
    "outputId": "c2140928-6ad3-4ef4-9ac8-2d0c0772ae07"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date_Time</th>\n",
       "      <th>Direction_10m</th>\n",
       "      <th>Speed_10m</th>\n",
       "      <th>Temperature_10m</th>\n",
       "      <th>Pressure_seaLevel</th>\n",
       "      <th>Air_Density_10m</th>\n",
       "      <th>Direction_50m</th>\n",
       "      <th>Speed_50m</th>\n",
       "      <th>Temperature_50m</th>\n",
       "      <th>Air_Density_50m</th>\n",
       "      <th>Direction_100m</th>\n",
       "      <th>Speed_100m</th>\n",
       "      <th>Temperature_100m</th>\n",
       "      <th>Air_Density_100m</th>\n",
       "      <th>Direction_150m</th>\n",
       "      <th>Speed_150m</th>\n",
       "      <th>Temperature_150m</th>\n",
       "      <th>Air_Density_150m</th>\n",
       "      <th>Park_Power_[KW]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-07 20:15:00</td>\n",
       "      <td>0.476323</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>0.218502</td>\n",
       "      <td>0.782227</td>\n",
       "      <td>0.752212</td>\n",
       "      <td>0.473538</td>\n",
       "      <td>0.127679</td>\n",
       "      <td>0.218502</td>\n",
       "      <td>0.752212</td>\n",
       "      <td>0.484680</td>\n",
       "      <td>0.047716</td>\n",
       "      <td>0.218502</td>\n",
       "      <td>0.752212</td>\n",
       "      <td>0.515320</td>\n",
       "      <td>0.019504</td>\n",
       "      <td>0.218502</td>\n",
       "      <td>0.752212</td>\n",
       "      <td>0.690394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-07 20:30:00</td>\n",
       "      <td>0.479109</td>\n",
       "      <td>0.116923</td>\n",
       "      <td>0.214832</td>\n",
       "      <td>0.781496</td>\n",
       "      <td>0.755162</td>\n",
       "      <td>0.479109</td>\n",
       "      <td>0.135714</td>\n",
       "      <td>0.214832</td>\n",
       "      <td>0.755162</td>\n",
       "      <td>0.473538</td>\n",
       "      <td>0.053299</td>\n",
       "      <td>0.214832</td>\n",
       "      <td>0.755162</td>\n",
       "      <td>0.459610</td>\n",
       "      <td>0.024823</td>\n",
       "      <td>0.214832</td>\n",
       "      <td>0.755162</td>\n",
       "      <td>0.690394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-07 20:45:00</td>\n",
       "      <td>0.481894</td>\n",
       "      <td>0.127692</td>\n",
       "      <td>0.211009</td>\n",
       "      <td>0.780216</td>\n",
       "      <td>0.758112</td>\n",
       "      <td>0.481894</td>\n",
       "      <td>0.147321</td>\n",
       "      <td>0.211009</td>\n",
       "      <td>0.758112</td>\n",
       "      <td>0.462396</td>\n",
       "      <td>0.061421</td>\n",
       "      <td>0.211009</td>\n",
       "      <td>0.758112</td>\n",
       "      <td>0.431755</td>\n",
       "      <td>0.032624</td>\n",
       "      <td>0.211009</td>\n",
       "      <td>0.758112</td>\n",
       "      <td>0.690394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-07 21:00:00</td>\n",
       "      <td>0.484680</td>\n",
       "      <td>0.140769</td>\n",
       "      <td>0.207187</td>\n",
       "      <td>0.778387</td>\n",
       "      <td>0.764012</td>\n",
       "      <td>0.487465</td>\n",
       "      <td>0.161607</td>\n",
       "      <td>0.207187</td>\n",
       "      <td>0.764012</td>\n",
       "      <td>0.459610</td>\n",
       "      <td>0.072589</td>\n",
       "      <td>0.207187</td>\n",
       "      <td>0.764012</td>\n",
       "      <td>0.417827</td>\n",
       "      <td>0.042908</td>\n",
       "      <td>0.207187</td>\n",
       "      <td>0.764012</td>\n",
       "      <td>0.690394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-07 21:15:00</td>\n",
       "      <td>0.487465</td>\n",
       "      <td>0.155385</td>\n",
       "      <td>0.203364</td>\n",
       "      <td>0.776376</td>\n",
       "      <td>0.766962</td>\n",
       "      <td>0.490251</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.203364</td>\n",
       "      <td>0.766962</td>\n",
       "      <td>0.456825</td>\n",
       "      <td>0.084772</td>\n",
       "      <td>0.203364</td>\n",
       "      <td>0.766962</td>\n",
       "      <td>0.412256</td>\n",
       "      <td>0.054610</td>\n",
       "      <td>0.203364</td>\n",
       "      <td>0.766962</td>\n",
       "      <td>0.690394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date_Time  Direction_10m  Speed_10m  Temperature_10m  \\\n",
       "0  2017-01-07 20:15:00       0.476323   0.110000         0.218502   \n",
       "1  2017-01-07 20:30:00       0.479109   0.116923         0.214832   \n",
       "2  2017-01-07 20:45:00       0.481894   0.127692         0.211009   \n",
       "3  2017-01-07 21:00:00       0.484680   0.140769         0.207187   \n",
       "4  2017-01-07 21:15:00       0.487465   0.155385         0.203364   \n",
       "\n",
       "   Pressure_seaLevel  Air_Density_10m  Direction_50m  Speed_50m  \\\n",
       "0           0.782227         0.752212       0.473538   0.127679   \n",
       "1           0.781496         0.755162       0.479109   0.135714   \n",
       "2           0.780216         0.758112       0.481894   0.147321   \n",
       "3           0.778387         0.764012       0.487465   0.161607   \n",
       "4           0.776376         0.766962       0.490251   0.178571   \n",
       "\n",
       "   Temperature_50m  Air_Density_50m  Direction_100m  Speed_100m  \\\n",
       "0         0.218502         0.752212        0.484680    0.047716   \n",
       "1         0.214832         0.755162        0.473538    0.053299   \n",
       "2         0.211009         0.758112        0.462396    0.061421   \n",
       "3         0.207187         0.764012        0.459610    0.072589   \n",
       "4         0.203364         0.766962        0.456825    0.084772   \n",
       "\n",
       "   Temperature_100m  Air_Density_100m  Direction_150m  Speed_150m  \\\n",
       "0          0.218502          0.752212        0.515320    0.019504   \n",
       "1          0.214832          0.755162        0.459610    0.024823   \n",
       "2          0.211009          0.758112        0.431755    0.032624   \n",
       "3          0.207187          0.764012        0.417827    0.042908   \n",
       "4          0.203364          0.766962        0.412256    0.054610   \n",
       "\n",
       "   Temperature_150m  Air_Density_150m  Park_Power_[KW]  \n",
       "0          0.218502          0.752212         0.690394  \n",
       "1          0.214832          0.755162         0.690394  \n",
       "2          0.211009          0.758112         0.690394  \n",
       "3          0.207187          0.764012         0.690394  \n",
       "4          0.203364          0.766962         0.690394  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# drive_path = 'drive/My Drive/Colab Notebooks/WP_Datasets_Normalized/'\n",
    "# Cebrina's   Path: C:\\Users\\cebri\\Documents\\Wind Power Estimation\\Data\n",
    "# Guillermo's Path: C:\\DTU\\\\02456 - Deep Learning\\Project\\Datasets\n",
    "# Tomi's      Path: C:\\Users\\PC\\Documents\\GitHub\\WindPower_Estimation\n",
    "\n",
    "dataPath = r'C:\\Users\\PC\\Documents\\GitHub\\WindPower_Estimation'\n",
    "\n",
    "dataset_train_1 = pd.read_csv(dataPath+'\\Case1\\Dataset_Train_1.csv', )\n",
    "dataset_test_1 = pd.read_csv(dataPath+'\\Case1\\Dataset_Test_1.csv')\n",
    "\n",
    "dataset_train_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xSItPJipBaZ5"
   },
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "wb-Z7wNKUJko"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e31mswiSBEEB"
   },
   "source": [
    "## Data Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Full set\n",
    "training_set_X = dataset_train_1[['Speed_50m', 'Direction_50m'  ]]\n",
    "training_set_Y = dataset_train_1[['Park_Power_[KW]'  ]]\n",
    "\n",
    "test_set_X = dataset_test_1[[ 'Speed_50m', 'Direction_50m' ]]\n",
    "test_set_Y = dataset_test_1[[ 'Park_Power_[KW]'  ]]\n",
    "\n",
    "training_set_X_numpy = training_set_X.to_numpy()\n",
    "training_set_Y_numpy = training_set_Y.to_numpy()\n",
    "test_set_X_numpy = test_set_X.to_numpy()\n",
    "test_set_Y_numpy = test_set_Y.to_numpy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_ts_multi_data_prep(dataset, targetset, start, window, horizon=1, end= None):\n",
    "    '''\n",
    "    dataset: the datas that we know and want to predict from, numpy array\n",
    "    target:  the datas that we want to predict, used for error calculation\n",
    "    start:   with which indice to start\n",
    "    window:  how many timestamp we use from the the past, how wide is the window\n",
    "    horizon: how many future target values are demanded\n",
    "    '''\n",
    "    X = []\n",
    "    y = []\n",
    "    start = start + window\n",
    "    if end is None:\n",
    "        end = dataset.shape[0] - horizon\n",
    "    \n",
    "    for i in range(start, end):\n",
    "        indices = range(i-window, i)\n",
    "        X.append(dataset[indices])\n",
    "        indicey = range(i, i+horizon )\n",
    "        y.append(targetset[indicey])\n",
    "    return np.array(X), np.array(y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_window = 96\n",
    "how_many_PowerkW_values = 1\n",
    "TRAIN_SPLIT = 30000\n",
    "x_train, y_train = custom_ts_multi_data_prep(training_set_X_numpy,  training_set_Y_numpy,  start=0,  window=hist_window,  horizon=how_many_PowerkW_values)\n",
    "x_test,  y_test  = custom_ts_multi_data_prep(test_set_X_numpy,  test_set_Y_numpy,  start=0,  window=hist_window,  horizon=how_many_PowerkW_values)\n",
    "\n",
    "\n",
    "trainX = Variable(torch.Tensor(np.array(x_train)) )\n",
    "trainY = Variable(torch.Tensor(np.array(y_train)) )\n",
    "testX = Variable(torch.Tensor(np.array(x_test)) )\n",
    "testY = Variable(torch.Tensor(np.array(y_test)) )\n",
    " \n",
    "# #  ----Debug----   \n",
    "# print('Multiple window of past history\\n')\n",
    "# print( trainX[1]  )\n",
    "# print('Target horizon\\n')\n",
    "# print(trainY.shape ) \n",
    "\n",
    "# print( len(trainX[1]))\n",
    "# print(trainX[1].reshape(-1) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self,  num_features, num_hidden, seq_len, batch_size,  num_layers=1):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = num_features\n",
    "        self.seq_len = seq_len\n",
    "        self.hidden_size = num_hidden\n",
    "#         self.seq_length = seq_length\n",
    "        \n",
    "    \n",
    "        # Recurrent layer\n",
    "        self.lstm = nn.LSTM(input_size=num_features, hidden_size=num_hidden,\n",
    "                            num_layers=num_layers, dropout=0.02, batch_first=True ) #dropout=0.1\n",
    "        \n",
    "        # Output layer\n",
    "#         self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
    "#         self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, 1) #output_size=1\n",
    "\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initializing hidden state for first input with zeros\n",
    "        h_0 = Variable(torch.zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_size)).to('cuda')  #.cuda()\n",
    "        \n",
    "#         print( x.size(0) )\n",
    "#         print('================')\n",
    "        # Initializing cell state for first input with zeros\n",
    "        c_0 = Variable(torch.zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_size)).to('cuda')   #.cuda()\n",
    "        \n",
    "        # Propagate input through LSTM\n",
    "#         print('test::::')\n",
    "#         print(h_0.shape)\n",
    "#         print(c_0.shape)\n",
    "#         print(x.shape)\n",
    "        #print(x.view(seq_len, 1))\n",
    "        \n",
    "#         print(f'input size: {x.shape}' )\n",
    "        x_output, (hn, cn) = self.lstm( x, (h_0, c_0))   #x.view( len(x), self.seq_len, self.hidden_size)\n",
    "        \n",
    "#         print( x_output.shape )\n",
    "#         print( x_output[0:128 , 0:1, :1].shape)\n",
    "        \n",
    "        lstm_out = x_output[-1].view(-1, self.hidden_size)  #h_out[-1] should be the results from the last LSTM layer\n",
    "#         last_time_step = lstm_out.view(self.seq_len, len(x), self.n_hidden)[-1]\n",
    "#         x_output = self.relu( self.fc1( x_output ) )\n",
    "        y_pred = self.fc2(x_output )\n",
    "    \n",
    "        return torch.squeeze( y_pred[0:y_pred.size(0) ,-1, :1], 1 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(117764, 2)\n",
      "0 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "1 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "2 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "3 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "4 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "5 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "6 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "7 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "8 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "9 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "10 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "11 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "12 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "13 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "14 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "15 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "16 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "17 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "18 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "19 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "20 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "21 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "22 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "23 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "24 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "25 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "26 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "27 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "28 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "29 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "30 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "31 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "32 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "33 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "34 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "35 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "36 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "37 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "38 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "39 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "40 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "41 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "42 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "43 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "44 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "45 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "46 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "47 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "48 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "49 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "50 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "51 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "52 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "53 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "54 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "55 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "56 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "57 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "58 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "59 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "60 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "61 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "62 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "63 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "64 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "65 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "66 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "67 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "68 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "69 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "70 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "71 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "72 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "73 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "74 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "75 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "76 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "77 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "78 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "79 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "80 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "81 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "82 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "83 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "84 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "85 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "86 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "87 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "88 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "89 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "90 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "91 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "92 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "93 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "94 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "95 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "96 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "97 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "98 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "99 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "100 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "101 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "102 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "103 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "104 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "105 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "106 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "107 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "108 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "109 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "110 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "111 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "112 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "113 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "114 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "115 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "116 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "117 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "118 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "119 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "120 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "121 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "122 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "123 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "124 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "125 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "126 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "127 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "128 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "129 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "130 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "131 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "132 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "133 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "134 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "135 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "136 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "137 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "138 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "139 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "140 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "141 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "142 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "143 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "144 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "145 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "146 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "147 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "148 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "149 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "150 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "151 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "152 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "153 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "154 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "155 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "156 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "157 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "158 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "159 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "160 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "161 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "162 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "163 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "164 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "165 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "166 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "167 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "168 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "169 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "170 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "171 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "172 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "173 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "174 torch.Size([672, 8, 2]) torch.Size([672, 1])\n",
      "175 torch.Size([157, 8, 2]) torch.Size([157, 1])\n"
     ]
    }
   ],
   "source": [
    "class TimeseriesDataset(torch.utils.data.Dataset):   \n",
    "    def __init__(self, X, y, seq_len=1):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.__len__() - (self.seq_len-1)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.X[index:index+self.seq_len], self.y[index+self.seq_len-1])\n",
    "    \n",
    "    \n",
    "batch_siz = 672\n",
    "print( training_set_X_numpy.shape)\n",
    "\n",
    "train_dataset = TimeseriesDataset(training_set_X_numpy, training_set_Y_numpy, seq_len=8)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_siz, shuffle = False)\n",
    "\n",
    "for i, d in enumerate(train_loader):\n",
    "    print(i, d[0].shape, d[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape training: (117600, 2)\n",
      "training remainder: 0\n",
      "training remainder: 0\n",
      "shape training2: (117600, 2)\n",
      "\n",
      "shape test: (28896, 2)\n",
      "test remainder: 0\n",
      "test remainder: 0\n",
      "shape training2: (28896, 2)\n",
      "\n",
      "run on cuda\n",
      "True\n",
      "LSTM(\n",
      "  (lstm): LSTM(2, 100, batch_first=True, dropout=0.02)\n",
      "  (fc2): Linear(in_features=100, out_features=1, bias=True)\n",
      ")\n",
      "new epoch, number0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.02 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:520: UserWarning: Using a target size (torch.Size([672, 1])) that is different to the input size (torch.Size([672])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:520: UserWarning: Using a target size (torch.Size([577, 1])) that is different to the input size (torch.Size([577])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train loss: 0.000139816,  test loss: 0.000183342\n",
      "new epoch, number1\n",
      "Epoch: 1, train loss: 0.000132324,  test loss: 0.000186160\n",
      "new epoch, number2\n",
      "Epoch: 2, train loss: 0.000130607,  test loss: 0.000186175\n",
      "new epoch, number3\n",
      "Epoch: 3, train loss: 0.000129733,  test loss: 0.000185649\n",
      "new epoch, number4\n",
      "Epoch: 4, train loss: 0.000129133,  test loss: 0.000185491\n",
      "new epoch, number5\n",
      "Epoch: 5, train loss: 0.000128629,  test loss: 0.000185028\n",
      "new epoch, number6\n",
      "Epoch: 6, train loss: 0.000128226,  test loss: 0.000184936\n",
      "new epoch, number7\n",
      "Epoch: 7, train loss: 0.000127810,  test loss: 0.000184478\n",
      "new epoch, number8\n",
      "Epoch: 8, train loss: 0.000127466,  test loss: 0.000184305\n",
      "new epoch, number9\n",
      "Epoch: 9, train loss: 0.000127156,  test loss: 0.000184294\n",
      "new epoch, number10\n",
      "Epoch: 10, train loss: 0.000126868,  test loss: 0.000183943\n",
      "new epoch, number11\n",
      "Epoch: 11, train loss: 0.000126659,  test loss: 0.000183629\n",
      "new epoch, number12\n",
      "Epoch: 12, train loss: 0.000126491,  test loss: 0.000183694\n",
      "new epoch, number13\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "hist_window \n",
    "n_features = 2\n",
    "hidden_size = 100\n",
    "num_layers = 1\n",
    "\n",
    "batch_size = 672\n",
    "num_samples_train = len( training_set_X_numpy )\n",
    "num_batches_train = (num_samples_train // batch_size ) \n",
    "num_samples_test = len( test_set_X_numpy )\n",
    "num_batches_test = (num_samples_test // batch_size ) \n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "print(f'shape training: {( training_set_X_numpy.shape)}')\n",
    "remainder_train = len( training_set_X_numpy)%batch_size\n",
    "print(f'training remainder: {remainder_train}')\n",
    "\n",
    "for rem in range( remainder_train):\n",
    "    training_set_X_numpy = np.delete( training_set_X_numpy, 0, axis=0)\n",
    "    \n",
    "remainder2_train = len( training_set_X_numpy)%batch_size\n",
    "print(f'training remainder: {remainder2_train}')\n",
    "print(f'shape training2: { ( training_set_X_numpy.shape)}')\n",
    "#------------------------------------------------------------------------------------\n",
    "\n",
    "print()\n",
    "print(f'shape test: {( test_set_X_numpy.shape)}')\n",
    "remainder_test = len( test_set_X_numpy)%batch_size\n",
    "print(f'test remainder: {remainder_test}')\n",
    "for rem in range( remainder_test):\n",
    "    test_set_X_numpy = np.delete( test_set_X_numpy, 0, axis=0)\n",
    "remainder2_test = len( test_set_X_numpy)%batch_size\n",
    "print(f'test remainder: {remainder2_test}')\n",
    "print(f'shape training2: { ( test_set_X_numpy.shape)}')\n",
    "#------------------------------------------------------------------------------------\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "lstm = LSTM(num_features=n_features, num_hidden=hidden_size, seq_len=hist_window, num_layers=num_layers, batch_size=batch_size )\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    lstm.to(device)\n",
    "    print('run on cuda')\n",
    "    print( next(lstm.parameters()).is_cuda )\n",
    "print(lstm)\n",
    "\n",
    "criterion = torch.nn.MSELoss()    # mean-squared error for regression\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n",
    "#optimizer = torch.optim.SGD(lstm.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "train_loss_out = []\n",
    "test_loss_out = []\n",
    "\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    \n",
    "    train_dataset = TimeseriesDataset(training_set_X_numpy, training_set_Y_numpy, seq_len=96) \n",
    "    train_loader = iter( torch.utils.data.DataLoader(train_dataset, batch_size = batch_siz, shuffle = False))\n",
    "    \n",
    "    lstm.train()\n",
    "    print( f'new epoch, number{epoch}')\n",
    "    \n",
    "    batch_train_losses = 0\n",
    "    # For each sentence in training set\n",
    "    for index in range( num_batches_train ):\n",
    "        optimizer.zero_grad()\n",
    "        x, y = train_loader.next()\n",
    "        x= x.to('cuda')\n",
    "        y= y.to('cuda') \n",
    "        \n",
    "#         print(index)\n",
    "#         print( x.shape )\n",
    "\n",
    "        outputs = lstm(  x.float() )\n",
    "\n",
    "\n",
    "        # obtain the loss function\n",
    "#         print('++++++++')\n",
    "#         print(outputs.shape)\n",
    "# #         print(outputs )\n",
    "# #         print(outputs)\n",
    "#         print(y.shape)\n",
    "        train_loss = criterion(outputs, y.float())\n",
    "\n",
    "        train_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        batch_train_losses += train_loss.cpu().detach().numpy()\n",
    "\n",
    "#     training_loss = torch.mean(torch.stack (batch_train_losses)  )\n",
    "    train_loss_out.append( batch_train_losses/training_set_X_numpy.shape[0]  )\n",
    "        \n",
    "        \n",
    "#         optimizer.step()\n",
    "#         batch_train_losses.append(train_loss)\n",
    "    \n",
    "#     training_loss = torch.mean(torch.stack (batch_train_losses)  )\n",
    "#     train_loss_out.append(training_loss.item())\n",
    "    \n",
    "    \n",
    "    \n",
    "#         validation_loss = torch.mean(torch.stack (batch_val_losses)  )\n",
    "#         val_loss_out.append(validation_loss.item())\n",
    "#     train_loss.append(loss.item())\n",
    " \n",
    "    test_dataset = TimeseriesDataset( test_set_X_numpy, test_set_Y_numpy, seq_len=96) \n",
    "    test_loader = iter( torch.utils.data.DataLoader(test_dataset, batch_size = batch_siz, shuffle = False))\n",
    "    \n",
    "    lstm.eval()\n",
    "    with torch.no_grad():\n",
    "        batch_val_losses = 0\n",
    "        for index in range( num_batches_test ):\n",
    "            x_val, y_val = test_loader.next()\n",
    "            x_val = x_val.to('cuda')\n",
    "            y_val = y_val.to('cuda') \n",
    "\n",
    "            outputs = lstm(  x.float() )\n",
    "\n",
    "            val_loss = criterion(outputs, y.float())\n",
    "            batch_val_losses += val_loss.cpu().detach().numpy()\n",
    "        test_loss_out.append( batch_val_losses/test_set_X_numpy.shape[0]  )   \n",
    "        \n",
    "        \n",
    "#             batch_val_losses.append(val_loss)\n",
    "#         validation_loss = torch.mean(torch.stack (batch_val_losses)  )\n",
    "#         val_loss_out.append(validation_loss.item())\n",
    "    \n",
    "    print(\"Epoch: %d, train loss: %1.9f,  test loss: %1.9f\" % (epoch, train_loss_out[-1], test_loss_out[-1] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00018098672097992328, 0.00018319723728512014, 0.0001838602613480318, 0.0001838313570866982, 0.0001839168281072662, 0.00018399317438403764, 0.0001838025858714467, 0.00018381039124159586, 0.00018357087503231706, 0.0001836561132222414, 0.00018351350999659016, 0.00018354434342611405]\n"
     ]
    }
   ],
   "source": [
    "print( test_loss_out   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "epoch = np.arange( len(train_loss_out)   )\n",
    "plt.figure()\n",
    "plt.plot(epoch, train_loss_out, 'r', label='Training loss',)\n",
    "plt.plot(epoch, test_loss_out, 'b', label='Test loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch'), plt.ylabel('NLL')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Epoch: %d, train loss: %1.5f,  test loss: %1.5f\" % (epoch, loss.item(), val_loss_out[-1] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NabsV8O5BBd5"
   },
   "source": [
    "## Dataloading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--1LVbHOBSIy"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tdzFI5GJBUF5"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35ndYIwIKteS"
   },
   "source": [
    "## Testing for Park Power values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "59WXMPiv4ttI"
   },
   "outputs": [],
   "source": [
    "pred_testX = []\n",
    "\n",
    "# Predicting first values of the test data with last values of train data\n",
    "# Each iteration predicts exactly 1 value and adds it to the pred_testX array\n",
    "# When items for train data cannot be fed into the LSTM, items from pred_testX are used, one by one\n",
    "\n",
    "window_test_set = np.vstack((training_set[len(training_set)-seq_length:len(training_set)],[[0],[0]]))\n",
    "x, y = sliding_windows(window_test_set, seq_length)\n",
    "dataX = Variable(torch.Tensor(np.array(x)))\n",
    "\n",
    "lstm.eval()\n",
    "test_predict = lstm(dataX)\n",
    "data_predict = test_predict.data.numpy()\n",
    "pred_testX.append(data_predict[0])\n",
    "\n",
    "for i in range(1,seq_length):\n",
    "  window_test_set = np.vstack((training_set[len(training_set)-seq_length+i:len(training_set)],pred_testX,[[0],[0]]))\n",
    "  x, y = sliding_windows(window_test_set, seq_length)\n",
    "  dataX = Variable(torch.Tensor(np.array(x)))\n",
    "\n",
    "  lstm.eval()\n",
    "  test_predict = lstm(dataX)\n",
    "  data_predict = test_predict.data.numpy()\n",
    "  pred_testX.append(data_predict[0])\n",
    "\n",
    "# Predicting rest of the 96 values exclusively with predicted data from pred_testX\n",
    "for i in range(96-seq_length):\n",
    "  window_test_set = np.vstack((pred_testX[len(pred_testX)-seq_length:len(pred_testX)],[[0],[0]]))\n",
    "  x, y = sliding_windows(window_test_set, seq_length)\n",
    "  dataX = Variable(torch.Tensor(np.array(x)))\n",
    "\n",
    "  lstm.eval()\n",
    "  test_predict = lstm(dataX)\n",
    "  data_predict = test_predict.data.numpy()\n",
    "  pred_testX.append(data_predict[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294
    },
    "id": "CKEzO1jzKydL",
    "outputId": "8ca9c74b-c583-4476-ae96-49e4243c31d7"
   },
   "outputs": [],
   "source": [
    "# Plotting last train values with test values vs predicted values\n",
    "\n",
    "lstm.eval()\n",
    "train_predict = lstm(trainX)\n",
    "\n",
    "train_predict = train_predict.data.numpy()\n",
    "\n",
    "dataY_plot = np.vstack((training_set[len(training_set)-224:],test_set[0:97]))\n",
    "data_predict = np.vstack((train_predict[len(train_predict)-224:],pred_testX))\n",
    "\n",
    "plt.axvline(x=len(training_set[len(training_set)-224:]), c='r', linestyle='--')\n",
    "\n",
    "plt.plot(dataY_plot)\n",
    "plt.plot(data_predict)\n",
    "plt.suptitle('Time-Series Prediction')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "LSTM Prediction - Power data only - Better prediction Test set",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
