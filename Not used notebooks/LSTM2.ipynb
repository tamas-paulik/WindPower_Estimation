{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6FIi8t8NUTEJ"
   },
   "source": [
    "# Time Series Prediction with LSTM Using PyTorch\n",
    "\n",
    "This kernel is based on *datasets* from\n",
    "\n",
    "[Time Series Forecasting with the Long Short-Term Memory Network in Python](https://machinelearningmastery.com/time-series-forecasting-long-short-term-memory-network-python/)\n",
    "\n",
    "[Time Series Prediction with LSTM Recurrent Neural Networks in Python with Keras](https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9DqRvEBU4aL"
   },
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B5JAorx0dJ5i",
    "outputId": "ae778a19-cbe5-44cd-9735-53caed8543b0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "id": "lJfI5TLedR6q",
    "outputId": "c2140928-6ad3-4ef4-9ac8-2d0c0772ae07"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date_Time</th>\n",
       "      <th>Direction_10m</th>\n",
       "      <th>Speed_10m</th>\n",
       "      <th>Temperature_10m</th>\n",
       "      <th>Pressure_seaLevel</th>\n",
       "      <th>Air_Density_10m</th>\n",
       "      <th>Direction_50m</th>\n",
       "      <th>Speed_50m</th>\n",
       "      <th>Temperature_50m</th>\n",
       "      <th>Air_Density_50m</th>\n",
       "      <th>Direction_100m</th>\n",
       "      <th>Speed_100m</th>\n",
       "      <th>Temperature_100m</th>\n",
       "      <th>Air_Density_100m</th>\n",
       "      <th>Direction_150m</th>\n",
       "      <th>Speed_150m</th>\n",
       "      <th>Temperature_150m</th>\n",
       "      <th>Air_Density_150m</th>\n",
       "      <th>Park_Power_[KW]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-07 20:15:00</td>\n",
       "      <td>0.476323</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>0.218502</td>\n",
       "      <td>0.782227</td>\n",
       "      <td>0.752212</td>\n",
       "      <td>0.473538</td>\n",
       "      <td>0.127679</td>\n",
       "      <td>0.218502</td>\n",
       "      <td>0.752212</td>\n",
       "      <td>0.484680</td>\n",
       "      <td>0.047716</td>\n",
       "      <td>0.218502</td>\n",
       "      <td>0.752212</td>\n",
       "      <td>0.515320</td>\n",
       "      <td>0.019504</td>\n",
       "      <td>0.218502</td>\n",
       "      <td>0.752212</td>\n",
       "      <td>0.690394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-07 20:30:00</td>\n",
       "      <td>0.479109</td>\n",
       "      <td>0.116923</td>\n",
       "      <td>0.214832</td>\n",
       "      <td>0.781496</td>\n",
       "      <td>0.755162</td>\n",
       "      <td>0.479109</td>\n",
       "      <td>0.135714</td>\n",
       "      <td>0.214832</td>\n",
       "      <td>0.755162</td>\n",
       "      <td>0.473538</td>\n",
       "      <td>0.053299</td>\n",
       "      <td>0.214832</td>\n",
       "      <td>0.755162</td>\n",
       "      <td>0.459610</td>\n",
       "      <td>0.024823</td>\n",
       "      <td>0.214832</td>\n",
       "      <td>0.755162</td>\n",
       "      <td>0.690394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-07 20:45:00</td>\n",
       "      <td>0.481894</td>\n",
       "      <td>0.127692</td>\n",
       "      <td>0.211009</td>\n",
       "      <td>0.780216</td>\n",
       "      <td>0.758112</td>\n",
       "      <td>0.481894</td>\n",
       "      <td>0.147321</td>\n",
       "      <td>0.211009</td>\n",
       "      <td>0.758112</td>\n",
       "      <td>0.462396</td>\n",
       "      <td>0.061421</td>\n",
       "      <td>0.211009</td>\n",
       "      <td>0.758112</td>\n",
       "      <td>0.431755</td>\n",
       "      <td>0.032624</td>\n",
       "      <td>0.211009</td>\n",
       "      <td>0.758112</td>\n",
       "      <td>0.690394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-07 21:00:00</td>\n",
       "      <td>0.484680</td>\n",
       "      <td>0.140769</td>\n",
       "      <td>0.207187</td>\n",
       "      <td>0.778387</td>\n",
       "      <td>0.764012</td>\n",
       "      <td>0.487465</td>\n",
       "      <td>0.161607</td>\n",
       "      <td>0.207187</td>\n",
       "      <td>0.764012</td>\n",
       "      <td>0.459610</td>\n",
       "      <td>0.072589</td>\n",
       "      <td>0.207187</td>\n",
       "      <td>0.764012</td>\n",
       "      <td>0.417827</td>\n",
       "      <td>0.042908</td>\n",
       "      <td>0.207187</td>\n",
       "      <td>0.764012</td>\n",
       "      <td>0.690394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-07 21:15:00</td>\n",
       "      <td>0.487465</td>\n",
       "      <td>0.155385</td>\n",
       "      <td>0.203364</td>\n",
       "      <td>0.776376</td>\n",
       "      <td>0.766962</td>\n",
       "      <td>0.490251</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.203364</td>\n",
       "      <td>0.766962</td>\n",
       "      <td>0.456825</td>\n",
       "      <td>0.084772</td>\n",
       "      <td>0.203364</td>\n",
       "      <td>0.766962</td>\n",
       "      <td>0.412256</td>\n",
       "      <td>0.054610</td>\n",
       "      <td>0.203364</td>\n",
       "      <td>0.766962</td>\n",
       "      <td>0.690394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date_Time  Direction_10m  Speed_10m  Temperature_10m  \\\n",
       "0  2017-01-07 20:15:00       0.476323   0.110000         0.218502   \n",
       "1  2017-01-07 20:30:00       0.479109   0.116923         0.214832   \n",
       "2  2017-01-07 20:45:00       0.481894   0.127692         0.211009   \n",
       "3  2017-01-07 21:00:00       0.484680   0.140769         0.207187   \n",
       "4  2017-01-07 21:15:00       0.487465   0.155385         0.203364   \n",
       "\n",
       "   Pressure_seaLevel  Air_Density_10m  Direction_50m  Speed_50m  \\\n",
       "0           0.782227         0.752212       0.473538   0.127679   \n",
       "1           0.781496         0.755162       0.479109   0.135714   \n",
       "2           0.780216         0.758112       0.481894   0.147321   \n",
       "3           0.778387         0.764012       0.487465   0.161607   \n",
       "4           0.776376         0.766962       0.490251   0.178571   \n",
       "\n",
       "   Temperature_50m  Air_Density_50m  Direction_100m  Speed_100m  \\\n",
       "0         0.218502         0.752212        0.484680    0.047716   \n",
       "1         0.214832         0.755162        0.473538    0.053299   \n",
       "2         0.211009         0.758112        0.462396    0.061421   \n",
       "3         0.207187         0.764012        0.459610    0.072589   \n",
       "4         0.203364         0.766962        0.456825    0.084772   \n",
       "\n",
       "   Temperature_100m  Air_Density_100m  Direction_150m  Speed_150m  \\\n",
       "0          0.218502          0.752212        0.515320    0.019504   \n",
       "1          0.214832          0.755162        0.459610    0.024823   \n",
       "2          0.211009          0.758112        0.431755    0.032624   \n",
       "3          0.207187          0.764012        0.417827    0.042908   \n",
       "4          0.203364          0.766962        0.412256    0.054610   \n",
       "\n",
       "   Temperature_150m  Air_Density_150m  Park_Power_[KW]  \n",
       "0          0.218502          0.752212         0.690394  \n",
       "1          0.214832          0.755162         0.690394  \n",
       "2          0.211009          0.758112         0.690394  \n",
       "3          0.207187          0.764012         0.690394  \n",
       "4          0.203364          0.766962         0.690394  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# drive_path = 'drive/My Drive/Colab Notebooks/WP_Datasets_Normalized/'\n",
    "# Cebrina's   Path: C:\\Users\\cebri\\Documents\\Wind Power Estimation\\Data\n",
    "# Guillermo's Path: C:\\DTU\\\\02456 - Deep Learning\\Project\\Datasets\n",
    "# Tomi's      Path: C:\\Users\\PC\\Documents\\GitHub\\WindPower_Estimation\n",
    "\n",
    "dataPath = r'C:\\Users\\PC\\Documents\\GitHub\\WindPower_Estimation'\n",
    "\n",
    "dataset_train_1 = pd.read_csv(dataPath+'\\Case1\\Dataset_Train_1.csv', )\n",
    "dataset_test_1 = pd.read_csv(dataPath+'\\Case1\\Dataset_Test_1.csv')\n",
    "\n",
    "dataset_train_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xSItPJipBaZ5"
   },
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "wb-Z7wNKUJko"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e31mswiSBEEB"
   },
   "source": [
    "## Data Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Full set\n",
    "training_set_X = dataset_train_1[['Speed_50m', 'Direction_50m'  ]]\n",
    "training_set_Y = dataset_train_1[['Park_Power_[KW]'  ]]\n",
    "\n",
    "test_set_X = dataset_test_1[[ 'Speed_50m', 'Direction_50m' ]]\n",
    "test_set_Y = dataset_test_1[[ 'Park_Power_[KW]'  ]]\n",
    "\n",
    "training_set_X_numpy = training_set_X.to_numpy()\n",
    "training_set_Y_numpy = training_set_Y.to_numpy()\n",
    "test_set_X_numpy = test_set_X.to_numpy()\n",
    "test_set_Y_numpy = test_set_Y.to_numpy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_ts_multi_data_prep(dataset, targetset, start, window, horizon=1, end= None):\n",
    "    '''\n",
    "    dataset: the datas that we know and want to predict from, numpy array\n",
    "    target:  the datas that we want to predict, used for error calculation\n",
    "    start:   with which indice to start\n",
    "    window:  how many timestamp we use from the the past, how wide is the window\n",
    "    horizon: how many future target values are demanded\n",
    "    '''\n",
    "    X = []\n",
    "    y = []\n",
    "    start = start + window\n",
    "    if end is None:\n",
    "        end = dataset.shape[0] - horizon\n",
    "    \n",
    "    for i in range(start, end):\n",
    "        indices = range(i-window, i)\n",
    "        X.append(dataset[indices])\n",
    "        indicey = range(i, i+horizon )\n",
    "        y.append(targetset[indicey])\n",
    "    return np.array(X), np.array(y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_window = 96\n",
    "how_many_PowerkW_values = 1\n",
    "TRAIN_SPLIT = 30000\n",
    "x_train, y_train = custom_ts_multi_data_prep(training_set_X_numpy,  training_set_Y_numpy,  start=0,  window=hist_window,  horizon=how_many_PowerkW_values)\n",
    "x_test,  y_test  = custom_ts_multi_data_prep(test_set_X_numpy,  test_set_Y_numpy,  start=0,  window=hist_window,  horizon=how_many_PowerkW_values)\n",
    "\n",
    "\n",
    "trainX = Variable(torch.Tensor(np.array(x_train)) )\n",
    "trainY = Variable(torch.Tensor(np.array(y_train)) )\n",
    "testX = Variable(torch.Tensor(np.array(x_test)) )\n",
    "testY = Variable(torch.Tensor(np.array(y_test)) )\n",
    " \n",
    "# #  ----Debug----   \n",
    "# print('Multiple window of past history\\n')\n",
    "# print( trainX[1]  )\n",
    "# print('Target horizon\\n')\n",
    "# print(trainY.shape ) \n",
    "\n",
    "# print( len(trainX[1]))\n",
    "# print(trainX[1].reshape(-1) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self,  num_features, num_hidden, seq_len, batch_size,  num_layers=1):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = num_features\n",
    "        self.seq_len = seq_len\n",
    "        self.hidden_size = num_hidden\n",
    "#         self.seq_length = seq_length\n",
    "        \n",
    "    \n",
    "        # Recurrent layer\n",
    "        self.lstm = nn.LSTM(input_size=num_features, hidden_size=num_hidden,\n",
    "                            num_layers=num_layers, dropout=0.02, batch_first=True ) #dropout=0.1\n",
    "        \n",
    "        # Output layer\n",
    "#         self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, 1) #output_size=1\n",
    "\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initializing hidden state for first input with zeros\n",
    "        h_0 = Variable(torch.zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_size)).to('cuda')  #.cuda()\n",
    "        \n",
    "#         print( x.size(0) )\n",
    "#         print('================')\n",
    "        # Initializing cell state for first input with zeros\n",
    "        c_0 = Variable(torch.zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_size)).to('cuda')   #.cuda()\n",
    "        \n",
    "        # Propagate input through LSTM\n",
    "#         print('test::::')\n",
    "#         print(h_0.shape)\n",
    "#         print(c_0.shape)\n",
    "#         print(x.shape)\n",
    "        #print(x.view(seq_len, 1))\n",
    "        \n",
    "#         print(f'input size: {x.shape}' )\n",
    "        x_output, (hn, cn) = self.lstm( x, (h_0, c_0))   #x.view( len(x), self.seq_len, self.hidden_size)\n",
    "        \n",
    "#         print( x_output.shape )\n",
    "#         print( x_output[0:128 , 0:1, :1].shape)\n",
    "        \n",
    "        lstm_out = x_output[-1].view(-1, self.hidden_size)  #h_out[-1] should be the results from the last LSTM layer\n",
    "#         last_time_step = lstm_out.view(self.seq_len, len(x), self.n_hidden)[-1]\n",
    "#         x_output = self.relu( self.fc1( x_output ) )\n",
    "        y_pred = self.relu( self.fc2(x_output ) )\n",
    "    \n",
    "        return torch.squeeze( y_pred[0:y_pred.size(0) ,-1, :1], 1 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(117764, 2)\n"
     ]
    }
   ],
   "source": [
    "class TimeseriesDataset(torch.utils.data.Dataset):   \n",
    "    def __init__(self, X, y, seq_len=1):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.__len__() - (self.seq_len-1)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.X[index:index+self.seq_len], self.y[index+self.seq_len-1])\n",
    "    \n",
    "    \n",
    "batch_siz = 672\n",
    "print( training_set_X_numpy.shape)\n",
    "\n",
    "train_dataset = TimeseriesDataset(training_set_X_numpy, training_set_Y_numpy, seq_len=8)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_siz, shuffle = False)\n",
    "\n",
    "# for i, d in enumerate(train_loader):\n",
    "#     print(i, d[0].shape, d[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape training: (117764, 2)\n",
      "training remainder: 164\n",
      "training remainder: 0\n",
      "shape training2: (117600, 2)\n",
      "\n",
      "shape test: (29441, 2)\n",
      "test remainder: 545\n",
      "test remainder: 0\n",
      "shape training2: (28896, 2)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.02 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run on cuda\n",
      "True\n",
      "LSTM(\n",
      "  (lstm): LSTM(2, 20, batch_first=True, dropout=0.02)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=20, out_features=1, bias=True)\n",
      ")\n",
      "new epoch, number0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:520: UserWarning: Using a target size (torch.Size([672, 1])) that is different to the input size (torch.Size([672])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:520: UserWarning: Using a target size (torch.Size([577, 1])) that is different to the input size (torch.Size([577])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train loss: 0.000138042,  test loss: 0.000181583\n",
      "new epoch, number1\n",
      "Epoch: 1, train loss: 0.000132727,  test loss: 0.000181920\n",
      "new epoch, number2\n",
      "Epoch: 2, train loss: 0.000131388,  test loss: 0.000182332\n",
      "new epoch, number3\n",
      "Epoch: 3, train loss: 0.000130544,  test loss: 0.000182570\n",
      "new epoch, number4\n",
      "Epoch: 4, train loss: 0.000129921,  test loss: 0.000182723\n",
      "new epoch, number5\n",
      "Epoch: 5, train loss: 0.000129433,  test loss: 0.000182816\n",
      "new epoch, number6\n",
      "Epoch: 6, train loss: 0.000129034,  test loss: 0.000182867\n",
      "new epoch, number7\n",
      "Epoch: 7, train loss: 0.000128698,  test loss: 0.000182888\n",
      "new epoch, number8\n",
      "Epoch: 8, train loss: 0.000128411,  test loss: 0.000182907\n",
      "new epoch, number9\n",
      "Epoch: 9, train loss: 0.000128161,  test loss: 0.000182890\n",
      "new epoch, number10\n",
      "Epoch: 10, train loss: 0.000127965,  test loss: 0.000182888\n",
      "new epoch, number11\n",
      "Epoch: 11, train loss: 0.000127782,  test loss: 0.000182939\n",
      "new epoch, number12\n",
      "Epoch: 12, train loss: 0.000127566,  test loss: 0.000182915\n",
      "new epoch, number13\n",
      "Epoch: 13, train loss: 0.000127410,  test loss: 0.000182894\n",
      "new epoch, number14\n",
      "Epoch: 14, train loss: 0.000127264,  test loss: 0.000182875\n",
      "new epoch, number15\n",
      "Epoch: 15, train loss: 0.000127129,  test loss: 0.000182859\n",
      "new epoch, number16\n",
      "Epoch: 16, train loss: 0.000127003,  test loss: 0.000182845\n",
      "new epoch, number17\n",
      "Epoch: 17, train loss: 0.000126885,  test loss: 0.000182833\n",
      "new epoch, number18\n",
      "Epoch: 18, train loss: 0.000126775,  test loss: 0.000182821\n",
      "new epoch, number19\n",
      "Epoch: 19, train loss: 0.000126671,  test loss: 0.000182811\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "hist_window \n",
    "n_features = 2\n",
    "hidden_size = 20\n",
    "num_layers = 1\n",
    "\n",
    "batch_size = 672\n",
    "num_samples_train = len( training_set_X_numpy )\n",
    "num_batches_train = (num_samples_train // batch_size ) \n",
    "num_samples_test = len( test_set_X_numpy )\n",
    "num_batches_test = (num_samples_test // batch_size ) \n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "print(f'shape training: {( training_set_X_numpy.shape)}')\n",
    "remainder_train = len( training_set_X_numpy)%batch_size\n",
    "print(f'training remainder: {remainder_train}')\n",
    "\n",
    "for rem in range( remainder_train):\n",
    "    training_set_X_numpy = np.delete( training_set_X_numpy, 0, axis=0)\n",
    "    \n",
    "remainder2_train = len( training_set_X_numpy)%batch_size\n",
    "print(f'training remainder: {remainder2_train}')\n",
    "print(f'shape training2: { ( training_set_X_numpy.shape)}')\n",
    "#------------------------------------------------------------------------------------\n",
    "\n",
    "print()\n",
    "print(f'shape test: {( test_set_X_numpy.shape)}')\n",
    "remainder_test = len( test_set_X_numpy)%batch_size\n",
    "print(f'test remainder: {remainder_test}')\n",
    "for rem in range( remainder_test):\n",
    "    test_set_X_numpy = np.delete( test_set_X_numpy, 0, axis=0)\n",
    "remainder2_test = len( test_set_X_numpy)%batch_size\n",
    "print(f'test remainder: {remainder2_test}')\n",
    "print(f'shape training2: { ( test_set_X_numpy.shape)}')\n",
    "#------------------------------------------------------------------------------------\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "lstm = LSTM(num_features=n_features, num_hidden=hidden_size, seq_len=hist_window, num_layers=num_layers, batch_size=batch_size )\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    lstm.to(device)\n",
    "    print('run on cuda')\n",
    "    print( next(lstm.parameters()).is_cuda )\n",
    "print(lstm)\n",
    "\n",
    "criterion = torch.nn.MSELoss()    # mean-squared error for regression\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n",
    "#optimizer = torch.optim.SGD(lstm.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "train_loss_out = []\n",
    "test_loss_out = []\n",
    "\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    \n",
    "    train_dataset = TimeseriesDataset(training_set_X_numpy, training_set_Y_numpy, seq_len=96) \n",
    "    train_loader = iter( torch.utils.data.DataLoader(train_dataset, batch_size = batch_siz, shuffle = False))\n",
    "    \n",
    "    lstm.train()\n",
    "    print( f'new epoch, number{epoch}')\n",
    "    \n",
    "    batch_train_losses = 0\n",
    "    # For each sentence in training set\n",
    "    for index in range( num_batches_train ):\n",
    "        optimizer.zero_grad()\n",
    "        x, y = train_loader.next()\n",
    "        x= x.to('cuda')\n",
    "        y= y.to('cuda') \n",
    "        \n",
    "#         print(index)\n",
    "#         print( x.shape )\n",
    "\n",
    "        outputs = lstm(  x.float() )\n",
    "\n",
    "\n",
    "        # obtain the loss function\n",
    "#         print('++++++++')\n",
    "#         print(outputs.shape)\n",
    "# #         print(outputs )\n",
    "# #         print(outputs)\n",
    "#         print(y.shape)\n",
    "        train_loss = criterion(outputs, y.float())\n",
    "\n",
    "        train_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        batch_train_losses += train_loss.cpu().detach().numpy()\n",
    "\n",
    "#     training_loss = torch.mean(torch.stack (batch_train_losses)  )\n",
    "    train_loss_out.append( batch_train_losses/training_set_X_numpy.shape[0]  )\n",
    "        \n",
    "        \n",
    "#         optimizer.step()\n",
    "#         batch_train_losses.append(train_loss)\n",
    "    \n",
    "#     training_loss = torch.mean(torch.stack (batch_train_losses)  )\n",
    "#     train_loss_out.append(training_loss.item())\n",
    "    \n",
    "    \n",
    "    \n",
    "#         validation_loss = torch.mean(torch.stack (batch_val_losses)  )\n",
    "#         val_loss_out.append(validation_loss.item())\n",
    "#     train_loss.append(loss.item())\n",
    " \n",
    "    test_dataset = TimeseriesDataset( test_set_X_numpy, test_set_Y_numpy, seq_len=96) \n",
    "    test_loader = iter( torch.utils.data.DataLoader(test_dataset, batch_size = batch_siz, shuffle = False))\n",
    "    \n",
    "    lstm.eval()\n",
    "    with torch.no_grad():\n",
    "        batch_val_losses = 0\n",
    "        for index in range( num_batches_test ):\n",
    "            x_val, y_val = test_loader.next()\n",
    "            x_val = x_val.to('cuda')\n",
    "            y_val = y_val.to('cuda') \n",
    "\n",
    "            outputs = lstm(  x.float() )\n",
    "\n",
    "            val_loss = criterion(outputs, y.float())\n",
    "            batch_val_losses += val_loss.cpu().detach().numpy()\n",
    "        test_loss_out.append( batch_val_losses/test_set_X_numpy.shape[0]  )   \n",
    "        \n",
    "        \n",
    "#             batch_val_losses.append(val_loss)\n",
    "#         validation_loss = torch.mean(torch.stack (batch_val_losses)  )\n",
    "#         val_loss_out.append(validation_loss.item())\n",
    "    \n",
    "    print(\"Epoch: %d, train loss: %1.9f,  test loss: %1.9f\" % (epoch, train_loss_out[-1], test_loss_out[-1] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0001815830778685354, 0.00018191997272272906, 0.0001823316173007091, 0.00018256983631068752, 0.00018272334931506996, 0.000182815705470386, 0.00018286661777113165, 0.00018288790514426573, 0.00018290701943139234, 0.0001828899784457116, 0.00018288787188274519, 0.00018293900592696098, 0.00018291460305807137, 0.00018289355960275446, 0.00018287496641278267, 0.0001828589897957586, 0.00018284514191604795, 0.00018283276863041378, 0.00018282135992887474, 0.0001828106940679607]\n"
     ]
    }
   ],
   "source": [
    "print( test_loss_out   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "epoch = np.arange( len(train_loss_out)   )\n",
    "plt.figure()\n",
    "plt.plot(epoch, train_loss_out, 'r', label='Training loss',)\n",
    "plt.plot(epoch, test_loss_out, 'b', label='Test loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch'), plt.ylabel('NLL')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NabsV8O5BBd5"
   },
   "source": [
    "## Dataloading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--1LVbHOBSIy"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tdzFI5GJBUF5"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35ndYIwIKteS"
   },
   "source": [
    "## Testing for Park Power values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "59WXMPiv4ttI"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 2 and the array at index 1 has size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-48ea34a93697>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# When items for train data cannot be fed into the LSTM, items from pred_testX are used, one by one\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mwindow_test_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_set_X\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_set_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_set_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msliding_windows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwindow_test_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mdataX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mvstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[1;34m(tup)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m         \u001b[0marrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0marrs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 283\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 2 and the array at index 1 has size 1"
     ]
    }
   ],
   "source": [
    "pred_testX = []\n",
    "\n",
    "# Predicting first values of the test data with last values of train data\n",
    "# Each iteration predicts exactly 1 value and adds it to the pred_testX array\n",
    "# When items for train data cannot be fed into the LSTM, items from pred_testX are used, one by one\n",
    "\n",
    "window_test_set = np.vstack( (training_set_X[len(training_set_X)-seq_length : len(training_set_X)], [[0],[0]]) )\n",
    "x, y = sliding_windows(window_test_set, seq_length)\n",
    "dataX = Variable(torch.Tensor(np.array(x)))\n",
    "\n",
    "lstm.eval()\n",
    "test_predict = lstm(dataX)\n",
    "data_predict = test_predict.data.numpy()\n",
    "pred_testX.append(data_predict[0])\n",
    "\n",
    "for i in range(1,seq_length):\n",
    "  window_test_set = np.vstack((training_set[len(training_set)-seq_length+i:len(training_set)],pred_testX,[[0],[0]]))\n",
    "  x, y = sliding_windows(window_test_set, seq_length)\n",
    "  dataX = Variable(torch.Tensor(np.array(x)))\n",
    "\n",
    "  lstm.eval()\n",
    "  test_predict = lstm(dataX)\n",
    "  data_predict = test_predict.data.numpy()\n",
    "  pred_testX.append(data_predict[0])\n",
    "\n",
    "# Predicting rest of the 96 values exclusively with predicted data from pred_testX\n",
    "for i in range(96-seq_length):\n",
    "  window_test_set = np.vstack((pred_testX[len(pred_testX)-seq_length:len(pred_testX)],[[0],[0]]))\n",
    "  x, y = sliding_windows(window_test_set, seq_length)\n",
    "  dataX = Variable(torch.Tensor(np.array(x)))\n",
    "\n",
    "  lstm.eval()\n",
    "  test_predict = lstm(dataX)\n",
    "  data_predict = test_predict.data.numpy()\n",
    "  pred_testX.append(data_predict[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294
    },
    "id": "CKEzO1jzKydL",
    "outputId": "8ca9c74b-c583-4476-ae96-49e4243c31d7"
   },
   "outputs": [],
   "source": [
    "# Plotting last train values with test values vs predicted values\n",
    "\n",
    "lstm.eval()\n",
    "train_predict = lstm(trainX)\n",
    "\n",
    "train_predict = train_predict.data.numpy()\n",
    "\n",
    "dataY_plot = np.vstack((training_set[len(training_set)-224:],test_set[0:97]))\n",
    "data_predict = np.vstack((train_predict[len(train_predict)-224:],pred_testX))\n",
    "\n",
    "plt.axvline(x=len(training_set[len(training_set)-224:]), c='r', linestyle='--')\n",
    "\n",
    "plt.plot(dataY_plot)\n",
    "plt.plot(data_predict)\n",
    "plt.suptitle('Time-Series Prediction')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "LSTM Prediction - Power data only - Better prediction Test set",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
