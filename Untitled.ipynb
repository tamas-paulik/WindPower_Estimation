{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18037386",
   "metadata": {
    "id": "18037386"
   },
   "outputs": [],
   "source": [
    "## Install pandas if you don't already have it (uncomment line below)\n",
    "# !pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a866ed90",
   "metadata": {
    "id": "a866ed90",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-f39eee8a0e01>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtseries\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0moffsets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomputation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0meval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m from pandas.core.reshape.api import (\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\computation\\api.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# flake8: noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomputation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0meval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "\n",
    "cm = 1/2.54  # centimeters in inches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba74712",
   "metadata": {
    "id": "0ba74712",
    "outputId": "7bff28d3-2e9c-4901-c414-ab7216a9b487",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#The datas are not uploaded to the repository, extract them to your base folder\n",
    "\n",
    "# Cebrina's   Path: C:\\Users\\cebri\\Documents\\Wind Power Estimation\\Data\n",
    "# Guillermo's Path: C:\\DTU\\\\02456 - Deep Learning\\Project\\Datasets\n",
    "# Tomi's      Path: C:\\Users\\PC\\Documents\\GitHub\\WindPower_Estimation\n",
    "\n",
    "dataPath = r\"C:\\Users\\PC\\Documents\\GitHub\\WindPower_Estimation\"\n",
    "\n",
    "# Reading Turbine power Curves\n",
    "turbine_1_power = pd.read_csv (r''+dataPath+'\\Case1\\Turbine_Info\\Power_Curve.txt', header=0, delim_whitespace=True)\n",
    "turbine_2_power = pd.read_csv (r''+dataPath+'\\Case2\\Turbine_Info\\Power_Curve.txt', header=0, delim_whitespace=True)\n",
    "turbine_3_power = pd.read_csv (r''+dataPath+'\\Case3\\Turbine_Info\\Power_Curve.txt', header=0, delim_whitespace=True)\n",
    "\n",
    "# Renaming of the headers\n",
    "turbine_1_power = turbine_1_power.rename(columns={ turbine_1_power.columns[0]: 'Case1 Turbine'})\n",
    "turbine_2_power = turbine_2_power.rename(columns={ turbine_2_power.columns[0]: 'Case2 Turbine'})\n",
    "turbine_3_power = turbine_3_power.rename(columns={ turbine_3_power.columns[0]: 'Case3 Turbine'})\n",
    "\n",
    "# Concatenation the Turbines to 1 dataframe\n",
    "all_turbines_power_curve_dataframe = pd.concat([turbine_1_power, turbine_2_power,turbine_3_power ], axis=1)\n",
    "\n",
    "# Interpolation for the missing values\n",
    "all_turbines_power_curve_dataframe.interpolate(inplace=True)\n",
    "\n",
    "# Creation of the power curve between 0m/s 3m/s wind speed datas\n",
    "all_turbines_power_curve_dataframe_init = pd.DataFrame({\"Case1 Turbine\":[0], \"Case2 Turbine\":[0], \"Case3 Turbine\":[0]} )\n",
    "all_turbines_power_curve_dataframe= all_turbines_power_curve_dataframe_init.append(all_turbines_power_curve_dataframe)\n",
    "\n",
    "# Renaming ofthe the index column\n",
    "all_turbines_power_curve_dataframe.index.name = 'Wind Speed'\n",
    "print( all_turbines_power_curve_dataframe )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0d7621",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e96bcd",
   "metadata": {
    "id": "65e96bcd",
    "outputId": "7c589294-fe63-4052-8251-4e1100e7c063",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reading Turbine info\n",
    "turbine_1_info = pd.read_csv (r''+dataPath+'\\Case1\\Turbine_Info\\Info.txt', header=None, delimiter=\":\", names =['Name', 'val'] )\n",
    "turbine_2_info = pd.read_csv (r''+dataPath+'\\Case2\\Turbine_Info\\Info.txt', header=None, delimiter=\":\", names =['Name', 'val'])\n",
    "turbine_3_info = pd.read_csv (r''+dataPath+'\\Case3\\Turbine_Info\\Info.txt', header=None, delimiter=\":\", names =['Name', 'val'])\n",
    "\n",
    "#Creation of the lists\n",
    "col_one_list = turbine_1_info.iloc[: , 0].tolist()\n",
    "turbine_1_info = turbine_1_info.iloc[: , 1].tolist()\n",
    "turbine_2_info = turbine_2_info.iloc[: , 1].tolist()\n",
    "turbine_3_info = turbine_3_info.iloc[: , 1].tolist()\n",
    "table = tuple(zip(*list(zip(turbine_1_info, turbine_2_info, turbine_3_info))))\n",
    "\n",
    "# Concatenation the Turbines to 1 dataframe\n",
    "all_turbines_info_dataframe = pd.DataFrame( (table) ,columns =col_one_list )\n",
    "#Increastion of the indexing\n",
    "all_turbines_info_dataframe.index += 1 \n",
    "\n",
    "print( all_turbines_info_dataframe)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e84cc5e",
   "metadata": {
    "id": "6e84cc5e",
    "outputId": "0cee1c2d-a5a0-4ec6-f08f-5ae4334b278d"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,5))\n",
    "\n",
    "ax1 = fig.add_subplot(121)\n",
    "all_turbines_power_curve_dataframe.plot(kind='line',y='Case1 Turbine',style=['r-'], ax=ax1)\n",
    "all_turbines_power_curve_dataframe.plot(kind='line',y='Case2 Turbine',style=['g-'], ax=ax1)\n",
    "all_turbines_power_curve_dataframe.plot(kind='line',y='Case3 Turbine',style=['b-'], ax=ax1)\n",
    "ax1.set_ylabel('Power KW')\n",
    "ax1.set_xlabel('Wind Speed m/s')\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "font_size=14\n",
    "bbox=[0, 0, 1, 1]\n",
    "ax2.axis('off')\n",
    "mpl_table = ax2.table(cellText = all_turbines_info_dataframe.values, rowLabels = all_turbines_info_dataframe.index, bbox=bbox, colLabels=all_turbines_info_dataframe.columns)\n",
    "mpl_table.auto_set_font_size(False)\n",
    "mpl_table.set_fontsize(font_size)\n",
    "\n",
    "fig.suptitle('Different types of turbines', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a7a6b3",
   "metadata": {
    "id": "70a7a6b3",
    "outputId": "350d865b-5c46-4d22-e2ee-baac3ec8eaa6"
   },
   "outputs": [],
   "source": [
    "# Reading the measurements\n",
    "measurement_case_1 = pd.read_csv (r''+dataPath+'\\Case1\\Proj_Measurements_Case1.csv', header=0)\n",
    "measurement_case_2 = pd.read_csv (r''+dataPath+'\\Case2\\Proj_Measurements_Case2.csv', header=0)\n",
    "measurement_case_3 = pd.read_csv (r''+dataPath+'\\Case3\\Proj_Measurements_Case3.csv', header=0)\n",
    "\n",
    "# Set indexing to the Date_time\n",
    "measurement_case_1['Date_Time']= pd.to_datetime(measurement_case_1['Date_Time'])\n",
    "measurement_case_2['Date_Time']= pd.to_datetime(measurement_case_2['Date_Time'])\n",
    "measurement_case_3['Date_Time']= pd.to_datetime(measurement_case_3['Date_Time'])\n",
    "measurement_case_1 = measurement_case_1.set_index('Date_Time')\n",
    "measurement_case_2 = measurement_case_2.set_index('Date_Time')\n",
    "measurement_case_3 = measurement_case_3.set_index('Date_Time')\n",
    "\n",
    "# Interpolation for the missing values (this should be done after splitting training/test sets)\n",
    "#measurement_case_1.interpolate(inplace=True)\n",
    "#measurement_case_2.interpolate(inplace=True)\n",
    "#measurement_case_3.interpolate(inplace=True)\n",
    "print(measurement_case_1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a5574e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the NWP data\n",
    "nwp_case_1 = pd.read_csv (r''+dataPath+'\\Case1\\Proj_NWP_Case1.csv', header=0)\n",
    "nwp_case_2 = pd.read_csv (r''+dataPath+'\\Case2\\Proj_NWP_Case2.csv', header=0)\n",
    "nwp_case_3 = pd.read_csv (r''+dataPath+'\\Case3\\Proj_NWP_Case3.csv', header=0)\n",
    "\n",
    "# Set indexing to the Date_time\n",
    "nwp_case_1['Date_Time']= pd.to_datetime(nwp_case_1['Date_Time'])\n",
    "nwp_case_2['Date_Time']= pd.to_datetime(nwp_case_2['Date_Time'])\n",
    "nwp_case_3['Date_Time']= pd.to_datetime(nwp_case_3['Date_Time'])\n",
    "nwp_case_1 = nwp_case_1.set_index('Date_Time')\n",
    "nwp_case_2 = nwp_case_2.set_index('Date_Time')\n",
    "nwp_case_3 = nwp_case_3.set_index('Date_Time')\n",
    "\n",
    "print(nwp_case_1.head() )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1b8ff2",
   "metadata": {
    "id": "5f1b8ff2",
    "outputId": "43ef361b-928b-43ff-b520-29d0dc59a13f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(100*cm,50*cm))\n",
    "ax1 = plt.gca()\n",
    "measurement_case_1.plot(kind='line', y='Speed_10m',style=['r-'], ax=ax1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a518b5",
   "metadata": {
    "id": "38a518b5",
    "outputId": "a270ae2e-990a-4a6e-f1e1-03ee8728706a"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(35,12))\n",
    "ax1 = plt.gca()\n",
    "measurement_case_1.plot(kind='line', y='Speed_30m',style=['r-'], ax=ax1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee26a9dd",
   "metadata": {
    "id": "ee26a9dd"
   },
   "outputs": [],
   "source": [
    "# Splitting measurement data into training and test sets\n",
    "# Method chosen: training set vs holdout (test) set\n",
    "# Because data is a time series, we need to respect the sequence (not pick data at random)\n",
    "# First portion of the data will be training set, last portion will be holdout\n",
    "train_test_proportion = 0.8\n",
    "\n",
    "train_test_threshold_1 = int(np.floor(len(measurement_case_1)*train_test_proportion))\n",
    "train_test_threshold_2 = int(np.floor(len(measurement_case_2)*train_test_proportion))\n",
    "train_test_threshold_3 = int(np.floor(len(measurement_case_3)*train_test_proportion))\n",
    "\n",
    "measurement_train_1 = measurement_case_1[0:train_test_threshold_1]\n",
    "measurement_train_2 = measurement_case_2[0:train_test_threshold_2]\n",
    "measurement_train_3 = measurement_case_3[0:train_test_threshold_3]\n",
    "\n",
    "measurement_test_1 = measurement_case_1[train_test_threshold_1:]\n",
    "measurement_test_2 = measurement_case_2[train_test_threshold_2:]\n",
    "measurement_test_3 = measurement_case_3[train_test_threshold_3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc44570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting NWP data into training and test sets\n",
    "# Making date-time limits between sets match\n",
    "# (not sure if this is the correct approach)\n",
    "\n",
    "nwp_train_1 = nwp_case_1[nwp_case_1.index <= max(measurement_train_1.index)]\n",
    "nwp_train_2 = nwp_case_2[nwp_case_2.index <= max(measurement_train_2.index)]\n",
    "nwp_train_3 = nwp_case_3[nwp_case_3.index <= max(measurement_train_3.index)]\n",
    "\n",
    "nwp_test_1 = nwp_case_1[nwp_case_1.index >= min(measurement_test_1.index)]\n",
    "nwp_test_2 = nwp_case_2[nwp_case_2.index >= min(measurement_test_2.index)]\n",
    "nwp_test_3 = nwp_case_3[nwp_case_3.index >= min(measurement_test_3.index)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4460c82d",
   "metadata": {
    "id": "4460c82d"
   },
   "outputs": [],
   "source": [
    "# Inspecting boxplots of training and test measurement data to detect outliers\n",
    "# Black circles in the plots are outliers\n",
    "\n",
    "fig = plt.figure(figsize=(18,5))\n",
    "\n",
    "ax1 = fig.add_subplot(131)\n",
    "measurement_train_1.boxplot(column =['Speed_10m','Speed_30m','Speed_65m','Speed_70m','Speed_80m'])\n",
    "\n",
    "ax2 = fig.add_subplot(132)\n",
    "measurement_train_2.boxplot(column =['Speed_10m','Speed_30m','Speed_65m','Speed_70m'])\n",
    "\n",
    "ax3 = fig.add_subplot(133)\n",
    "measurement_train_3.boxplot(column =['Speed_10m','Speed_30m','Speed_50m','Speed_80m'])\n",
    "\n",
    "fig.suptitle('Wind speed for cases 1, 2 and 3 (training sets)', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da81b3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,5))\n",
    "\n",
    "ax1 = fig.add_subplot(131)\n",
    "measurement_test_1.boxplot(column =['Speed_10m','Speed_30m','Speed_65m','Speed_70m','Speed_80m'])\n",
    "\n",
    "ax2 = fig.add_subplot(132)\n",
    "measurement_test_2.boxplot(column =['Speed_10m','Speed_30m','Speed_65m','Speed_70m'])\n",
    "\n",
    "ax3 = fig.add_subplot(133)\n",
    "measurement_test_3.boxplot(column =['Speed_10m','Speed_30m','Speed_50m','Speed_80m'])\n",
    "\n",
    "fig.suptitle('Wind speed for cases 1, 2 and 3 (test sets)', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58003f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,5))\n",
    "\n",
    "ax1 = fig.add_subplot(131)\n",
    "measurement_train_1.boxplot(column =['Direction_10m','Direction_30m','Direction_65m','Direction_70m'])\n",
    "\n",
    "ax2 = fig.add_subplot(132)\n",
    "measurement_train_2.boxplot(column =['Direction_10m','Direction_30m','Direction_65m','Direction_70m'])\n",
    "\n",
    "ax3 = fig.add_subplot(133)\n",
    "measurement_train_3.boxplot(column =['Direction_10m','Direction_30m','Direction_50m','Direction_80m'])\n",
    "\n",
    "fig.suptitle('Wind direction for cases 1, 2 and 3 (training sets)', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bb9718",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,5))\n",
    "\n",
    "ax1 = fig.add_subplot(131)\n",
    "measurement_test_1.boxplot(column =['Direction_10m','Direction_30m','Direction_65m','Direction_70m'])\n",
    "\n",
    "ax2 = fig.add_subplot(132)\n",
    "measurement_test_2.boxplot(column =['Direction_10m','Direction_30m','Direction_65m','Direction_70m'])\n",
    "\n",
    "ax3 = fig.add_subplot(133)\n",
    "measurement_test_3.boxplot(column =['Direction_10m','Direction_30m','Direction_50m','Direction_80m'])\n",
    "\n",
    "fig.suptitle('Wind direction for cases 1, 2 and 3 (test sets)', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2095a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,5))\n",
    "\n",
    "ax1 = fig.add_subplot(131)\n",
    "measurement_train_1.boxplot(column =['Park Power [KW]'])\n",
    "\n",
    "ax2 = fig.add_subplot(132)\n",
    "measurement_train_2.boxplot(column =['Park Power [KW]'])\n",
    "\n",
    "ax3 = fig.add_subplot(133)\n",
    "measurement_train_3.boxplot(column =['Park Power [KW]'])\n",
    "\n",
    "fig.suptitle('Park Power [KW] for cases 1, 2 and 3 (training sets)', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e48c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,5))\n",
    "\n",
    "ax1 = fig.add_subplot(131)\n",
    "measurement_test_1.boxplot(column =['Park Power [KW]'])\n",
    "\n",
    "ax2 = fig.add_subplot(132)\n",
    "measurement_test_2.boxplot(column =['Park Power [KW]'])\n",
    "\n",
    "ax3 = fig.add_subplot(133)\n",
    "measurement_test_3.boxplot(column =['Park Power [KW]'])\n",
    "\n",
    "fig.suptitle('Park Power [KW] for cases 1, 2 and 3 (test sets)', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddc4cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminating outliers from all columns\n",
    "# Calculations are done independently on training and test sets\n",
    "measurement_sets = [measurement_train_1, measurement_train_2, measurement_test_1, measurement_test_2]\n",
    "measurement_columns = ['Speed_10m','Speed_30m','Speed_65m','Speed_70m','Direction_10m','Direction_30m','Direction_65m','Direction_70m']\n",
    "\n",
    "for measurement_set in measurement_sets:\n",
    "    for measurement_column in measurement_columns:\n",
    "        q25, q75 = np.percentile(measurement_set[measurement_column].dropna(), 25), np.percentile(measurement_set[measurement_column].dropna(), 75)\n",
    "        iqr = q75 - q25\n",
    "        cut_off_low = q25 - iqr*1.5\n",
    "        cut_off_upp = q75 + iqr*1.5\n",
    "        measurement_set.loc[measurement_set[measurement_column] < cut_off_low, measurement_column] = np.nan\n",
    "        measurement_set.loc[measurement_set[measurement_column] > cut_off_upp, measurement_column] = np.nan\n",
    "\n",
    "measurement_sets = [measurement_train_3, measurement_test_3]\n",
    "measurement_columns = ['Speed_10m','Speed_30m','Speed_50m','Speed_80m','Direction_10m','Direction_30m','Direction_50m','Direction_80m']\n",
    "\n",
    "for measurement_set in measurement_sets:\n",
    "    for measurement_column in measurement_columns:\n",
    "        q25, q75 = np.percentile(measurement_set[measurement_column].dropna(), 25), np.percentile(measurement_set[measurement_column].dropna(), 75)\n",
    "        iqr = q75 - q25\n",
    "        cut_off_low = q25 - iqr*1.5\n",
    "        cut_off_upp = q75 + iqr*1.5\n",
    "        measurement_set.loc[measurement_set[measurement_column] < cut_off_low, measurement_column] = np.nan\n",
    "        measurement_set.loc[measurement_set[measurement_column] > cut_off_upp, measurement_column] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85d5b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using linear interpolation to fill in missing values\n",
    "# (values can be missing from the original dataset or missing because they are outliers that were purged)\n",
    "# Applying interpolation to training and test sets separately\n",
    "\n",
    "measurement_train_1.interpolate(inplace=True)\n",
    "measurement_train_2.interpolate(inplace=True)\n",
    "measurement_train_3.interpolate(inplace=True)\n",
    "\n",
    "measurement_test_1.interpolate(inplace=True)\n",
    "measurement_test_2.interpolate(inplace=True)\n",
    "measurement_test_3.interpolate(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061e2c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting boxplots of training and test measurement after data cleaning\n",
    "\n",
    "fig = plt.figure(figsize=(18,5))\n",
    "\n",
    "ax1 = fig.add_subplot(131)\n",
    "measurement_train_1.boxplot(column =['Speed_10m','Speed_30m','Speed_65m','Speed_70m','Speed_80m'])\n",
    "\n",
    "ax2 = fig.add_subplot(132)\n",
    "measurement_train_2.boxplot(column =['Speed_10m','Speed_30m','Speed_65m','Speed_70m'])\n",
    "\n",
    "ax3 = fig.add_subplot(133)\n",
    "measurement_train_3.boxplot(column =['Speed_10m','Speed_30m','Speed_50m','Speed_80m'])\n",
    "\n",
    "fig.suptitle('Wind speed for cases 1, 2 and 3 (training sets)', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d506c95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Normalizing the data\n",
    "# For each column in the training datasets, values are normalized according to max and mins in each column\n",
    "# The same max and mins are used for the columns in the test datasets, in order to keep proportion\n",
    "\n",
    "set_columns = ['Speed_10m','Speed_30m','Speed_65m','Speed_70m', \\\n",
    "                'Direction_10m','Direction_30m','Direction_65m','Direction_70m','Park Power [KW]']\n",
    "\n",
    "# Measurement 1 (train and test)\n",
    "measurement_norm_1 = {}\n",
    "\n",
    "for column in set_columns:\n",
    "    maxvalue = max(measurement_train_1[column].dropna())\n",
    "    minvalue = min(measurement_train_1[column].dropna())\n",
    "    measurement_train_1[column] = (measurement_train_1[column]-minvalue)/(maxvalue-minvalue)\n",
    "    measurement_test_1[column] = (measurement_test_1[column]-minvalue)/(maxvalue-minvalue)\n",
    "    measurement_norm_1[column] = {\"max\": maxvalue, \"min\": minvalue}\n",
    "\n",
    "# Measurement 2 (train and test)\n",
    "measurement_norm_2 = {}\n",
    "\n",
    "for column in set_columns:\n",
    "    maxvalue = max(measurement_train_2[column].dropna())\n",
    "    minvalue = min(measurement_train_2[column].dropna())\n",
    "    measurement_train_2[column] = (measurement_train_2[column]-minvalue)/(maxvalue-minvalue)\n",
    "    measurement_test_2[column] = (measurement_test_2[column]-minvalue)/(maxvalue-minvalue)\n",
    "    measurement_norm_2[column] = {\"max\": maxvalue, \"min\": minvalue}\n",
    "\n",
    "set_columns = ['Speed_10m','Speed_30m','Speed_50m','Speed_80m', \\\n",
    "                'Direction_10m','Direction_30m','Direction_50m','Direction_80m','Park Power [KW]']\n",
    "\n",
    "# Measurement 3 (train and test)\n",
    "measurement_norm_3 = {}\n",
    "\n",
    "for column in set_columns:\n",
    "    maxvalue = max(measurement_train_3[column].dropna())\n",
    "    minvalue = min(measurement_train_3[column].dropna())\n",
    "    measurement_train_3[column] = (measurement_train_3[column]-minvalue)/(maxvalue-minvalue)\n",
    "    measurement_test_3[column] = (measurement_test_3[column]-minvalue)/(maxvalue-minvalue)\n",
    "    measurement_norm_3[column] = {\"max\": maxvalue, \"min\": minvalue}\n",
    "\n",
    "set_columns = ['Direction_10m','Speed_10m','Temperature_10m','Pressure_seaLevel', \\\n",
    "                'Air Density_10m','Direction_50m','Speed_50m','Temperature_50m','Air Density_50m', \\\n",
    "                'Direction_100m','Speed_100m','Temperature_100m','Air Density_100m','Direction_150m','Speed_150m', \\\n",
    "                'Temperature_150m','Air Density_150m']\n",
    "\n",
    "# NWP 1 (train and test)\n",
    "nwp_norm_1 = {}\n",
    "\n",
    "for column in set_columns:\n",
    "    maxvalue = max(nwp_train_1[column])\n",
    "    minvalue = min(nwp_train_1[column])\n",
    "    nwp_train_1[column] = (nwp_train_1[column]-minvalue)/(maxvalue-minvalue)\n",
    "    nwp_test_1[column] = (nwp_test_1[column]-minvalue)/(maxvalue-minvalue)\n",
    "    nwp_norm_1[column] = {\"max\": maxvalue, \"min\": minvalue}\n",
    "\n",
    "# NWP 2 (train and test)\n",
    "nwp_norm_2 = {}\n",
    "\n",
    "for column in set_columns:\n",
    "    maxvalue = max(nwp_train_2[column])\n",
    "    minvalue = min(nwp_train_2[column])\n",
    "    nwp_train_2[column] = (nwp_train_2[column]-minvalue)/(maxvalue-minvalue)\n",
    "    nwp_test_2[column] = (nwp_test_2[column]-minvalue)/(maxvalue-minvalue)\n",
    "    nwp_norm_2[column] = {\"max\": maxvalue, \"min\": minvalue}\n",
    "\n",
    "# NWP 3 (train and test)\n",
    "nwp_norm_3 = {}\n",
    "\n",
    "for column in set_columns:\n",
    "    maxvalue = max(nwp_train_3[column])\n",
    "    minvalue = min(nwp_train_3[column])\n",
    "    nwp_train_3[column] = (nwp_train_3[column]-minvalue)/(maxvalue-minvalue)\n",
    "    nwp_test_3[column] = (nwp_test_3[column]-minvalue)/(maxvalue-minvalue)\n",
    "    nwp_norm_3[column] = {\"max\": maxvalue, \"min\": minvalue}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ca98a1",
   "metadata": {},
   "source": [
    "# Merging Tables for data feeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440df1da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Merging the tables to match the time stamps\n",
    "new_train = pd.merge_asof( measurement_case_1['Park Power [KW]'], nwp_case_1, on=\"Date_Time\")\n",
    "len_of_power = len(measurement_case_1['Park Power [KW]']) \n",
    "len_of_train = len(nwp_case_1 ) \n",
    "print( len( new_train ))\n",
    "print( len_of_power )\n",
    "print( len_of_train )\n",
    "temp = 35724\n",
    "new_train = new_train.iloc[ 10 : , :]\n",
    "new_train = new_train.reset_index()\n",
    "print( new_train )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7857b82a",
   "metadata": {},
   "source": [
    "# FFNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda059b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#print(torch.__version__)\n",
    "#print(torch.version.cuda)\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load functions\n",
    "import torch.nn.init as init\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import Linear, GRU, Conv2d, Dropout, MaxPool2d, BatchNorm1d\n",
    "from torch.nn.functional import relu, elu, relu6, sigmoid, tanh, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac68c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of class for batch generation\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class TimeSeriesDataSet(Dataset):\n",
    "  \"\"\"\n",
    "  This is a custom dataset class. It can get more complex than this, but simplified so you can understand what's happening here without\n",
    "  getting bogged down by the preprocessing\n",
    "  \"\"\"\n",
    "  def __init__(self, X, Y, Z):\n",
    "    self.X = X\n",
    "    self.Y = Y\n",
    "    self.Z = Z\n",
    "    if len(self.X) != len(self.Y) :\n",
    "      raise Exception(\"The length of X does not match the length of Y\")\n",
    "    if len(self.X) != len(self.Z) :\n",
    "      raise Exception(\"The length of X does not match the length of Z\")\n",
    "    if len(self.Z) != len(self.Y) :\n",
    "      raise Exception(\"The length of Z does not match the length of Y\")\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    # note that this isn't randomly selecting. It's a simple get a single item that represents an x and y\n",
    "    _x = self.X[index]\n",
    "    _y = self.Y[index]\n",
    "    _z = self.Z[index]\n",
    "\n",
    "    return _x , _y , _z "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b4432c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the FFNN architecture\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, learn_bias=False ):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.ffn1 = nn.Linear(in_features =in_channels,\n",
    "                              out_features =out_channels,\n",
    "                              bias = learn_bias)\n",
    "        \n",
    "        self.ffn2 = nn.Linear(in_features =mid_channels,\n",
    "                              out_features =out_channels,\n",
    "                              bias = learn_bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x= ( self.ffn1(x))\n",
    "        return x  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad112d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### not implemented so far=========\n",
    "def accuracy(ys, ts):\n",
    "    predictions = torch.max(ys, 1)[1]\n",
    "    correct_prediction = torch.eq(predictions, ts)\n",
    "    return torch.mean(correct_prediction.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc81d078",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Putting to CUDA the dataset\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"Running GPU.\") if use_cuda else print(\"No GPU available.\")\n",
    "\n",
    "def get_variable(x):\n",
    "    \"\"\" Converts tensors to cuda, if available. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cuda()\n",
    "    return x\n",
    "\n",
    "def get_numpy(x):\n",
    "    \"\"\" Get numpy array for both cuda and not. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cpu().data.numpy()\n",
    "    return x.data.numpy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a73666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network creation and paramter modification\n",
    "LEARNING_RATE = 0.001\n",
    "criterion =  nn.MSELoss()       \n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "in_channels_setting = 16\n",
    "out_channels_setting = 16\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "net = Net(in_channels_setting, out_channels_setting )\n",
    "# if use_cuda:\n",
    "#     net.cuda()\n",
    "#     print('run on cuda')\n",
    "# print(net)\n",
    "\n",
    "#deleting the the extra data that doesnt fit the batch size\n",
    "modulus = len(new_train) %batch_size\n",
    "new_train_temp = new_train.iloc[ modulus: , :]\n",
    "new_train_temp = new_train_temp.reset_index()\n",
    "#print( new_train_temp )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a588221f",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91385fe3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###### setting hyperparameters and gettings epoch sizes\n",
    "num_epochs = 2\n",
    "num_samples_train = len( new_train )\n",
    "num_batches_train = (num_samples_train // batch_size ) \n",
    "num_samples_valid = len( new_train )\n",
    "num_batches_valid = (num_samples_valid // batch_size ) \n",
    "\n",
    "# setting up lists for handling loss/accuracy\n",
    "train_acc, train_loss = [], []\n",
    "valid_acc, valid_loss = [], []\n",
    "test_acc, test_loss = [], []\n",
    "\n",
    "losses_train_train = []\n",
    "Losses_train_valid = []\n",
    "Losses_test = []\n",
    "#get_slice = lambda i, size: range(i * size, (i + 1) * size)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward -> Backprob -> Update params\n",
    "    ## Train\n",
    "    train_loader = iter(DataLoader(TimeSeriesDataSet(new_train_temp['Direction_50m'], new_train_temp['Speed_50m'], new_train_temp['Speed_50m'] ), batch_size= batch_size_own, shuffle=False))\n",
    "\n",
    "    train_train_cur_loss = 0\n",
    "    net.train()\n",
    "    print( f'new epoch, number{epoch}')\n",
    "    for i in range(num_batches_train-10):\n",
    "        #print( f'{i} run')\n",
    "        optimizer.zero_grad()\n",
    "        x, y, z = train_loader.next()\n",
    "        output = net( y.float() )\n",
    "        \n",
    "        \n",
    "        # compute gradients given loss\n",
    "        # get the target ==> Z\n",
    "        batch_loss = criterion(output.float(), z.float())\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        train_train_cur_loss += batch_loss   \n",
    "    losses_train_train.append(train_train_cur_loss / batch_size)\n",
    "    \n",
    "    net.eval()\n",
    "    ### Evaluate training\n",
    "    train_loader = iter(DataLoader(TimeSeriesDataSet(new_train_temp['Direction_50m'], new_train_temp['Speed_50m'], new_train_temp['Speed_50m'] ), batch_size= batch_size_own, shuffle=False))\n",
    "    train_val_cur_loss = 0\n",
    "    for i in range(num_batches_train):\n",
    "        x, y, z = train_loader.next()\n",
    "        output = net( y.float() )\n",
    "        \n",
    "        batch_loss = criterion(output.float(), z.float())\n",
    "        \n",
    "        train_val_cur_loss += batch_loss.detach().numpy()\n",
    "    Losses_train_valid.append(train_val_cur_loss / batch_size)\n",
    "    \n",
    "    ### Evaluate validation\n",
    "    test_loader = iter(DataLoader(TimeSeriesDataSet(new_train_temp['Direction_50m'], new_train_temp['Speed_50m'], new_train_temp['Speed_50m'] ), batch_size= batch_size_own, shuffle=False))\n",
    "    test_cur_loss = 0\n",
    "    for i in range(num_batches_valid):\n",
    "        x, y, z = test_loader.next()\n",
    "        output = net( y.float() )\n",
    "        \n",
    "        batch_loss = criterion(output.float(), z.float())\n",
    "        \n",
    "        test_cur_loss += batch_loss.detach().numpy()\n",
    "    Losses_test.append(test_cur_loss / batch_size)\n",
    "    \n",
    "    \n",
    "epoch = np.arange(len(Losses_test))\n",
    "plt.figure()\n",
    "plt.plot( Losses_train_valid, 'r', Losses_test, 'b') #, epoch, Losses_test, 'b')\n",
    "plt.legend(['Train Loss','Validation Loss'])\n",
    "plt.xlabel('Updates'), plt.ylabel('Acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7c3b4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
