{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff1c7b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging Tables for data feeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ad06ec1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-74ea60c8b36c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m##Merging the tables to match the time stamps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnew_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge_asof\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mmeasurement_case_1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Park Power [KW]'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnwp_case_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Date_Time\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mlen_of_power\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmeasurement_case_1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Park Power [KW]'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlen_of_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnwp_case_1\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mnew_train\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "##Merging the tables to match the time stamps\n",
    "new_train = pd.merge_asof( measurement_case_1['Park Power [KW]'], nwp_case_1, on=\"Date_Time\")\n",
    "len_of_power = len(measurement_case_1['Park Power [KW]']) \n",
    "len_of_train = len(nwp_case_1 ) \n",
    "print( len( new_train ))\n",
    "print( len_of_power )\n",
    "print( len_of_train )\n",
    "temp = 35724\n",
    "new_train = new_train.iloc[ 10 : , :]\n",
    "new_train = new_train.reset_index()\n",
    "print( new_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c91d7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ddb04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#print(torch.__version__)\n",
    "#print(torch.version.cuda)\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load functions\n",
    "import torch.nn.init as init\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import Linear, GRU, Conv2d, Dropout, MaxPool2d, BatchNorm1d\n",
    "from torch.nn.functional import relu, elu, relu6, sigmoid, tanh, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe41532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of class for batch generation\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class TimeSeriesDataSet(Dataset):\n",
    "  \"\"\"\n",
    "  This is a custom dataset class. It can get more complex than this, but simplified so you can understand what's happening here without\n",
    "  getting bogged down by the preprocessing\n",
    "  \"\"\"\n",
    "  def __init__(self, X, Y, Z):\n",
    "    self.X = X\n",
    "    self.Y = Y\n",
    "    self.Z = Z\n",
    "    if len(self.X) != len(self.Y) :\n",
    "      raise Exception(\"The length of X does not match the length of Y\")\n",
    "    if len(self.X) != len(self.Z) :\n",
    "      raise Exception(\"The length of X does not match the length of Z\")\n",
    "    if len(self.Z) != len(self.Y) :\n",
    "      raise Exception(\"The length of Z does not match the length of Y\")\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    # note that this isn't randomly selecting. It's a simple get a single item that represents an x and y\n",
    "    _x = self.X[index]\n",
    "    _y = self.Y[index]\n",
    "    _z = self.Z[index]\n",
    "\n",
    "    return _x , _y , _z "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6220213f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the FFNN architecture\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, learn_bias=False ):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.ffn1 = nn.Linear(in_features =in_channels,\n",
    "                              out_features =out_channels,\n",
    "                              bias = learn_bias)\n",
    "        \n",
    "        self.ffn2 = nn.Linear(in_features =mid_channels,\n",
    "                              out_features =out_channels,\n",
    "                              bias = learn_bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x= ( self.ffn1(x))\n",
    "        return x  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e741f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### not implemented so far=========\n",
    "def accuracy(ys, ts):\n",
    "    predictions = torch.max(ys, 1)[1]\n",
    "    correct_prediction = torch.eq(predictions, ts)\n",
    "    return torch.mean(correct_prediction.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0508da59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting to CUDA the dataset\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"Running GPU.\") if use_cuda else print(\"No GPU available.\")\n",
    "\n",
    "def get_variable(x):\n",
    "    \"\"\" Converts tensors to cuda, if available. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cuda()\n",
    "    return x\n",
    "\n",
    "def get_numpy(x):\n",
    "    \"\"\" Get numpy array for both cuda and not. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cpu().data.numpy()\n",
    "    return x.data.numpy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861fd77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network creation and paramter modification\n",
    "LEARNING_RATE = 0.001\n",
    "criterion =  nn.MSELoss()       \n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "in_channels_setting = 16\n",
    "out_channels_setting = 16\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "net = Net(in_channels_setting, out_channels_setting )\n",
    "# if use_cuda:\n",
    "#     net.cuda()\n",
    "#     print('run on cuda')\n",
    "# print(net)\n",
    "\n",
    "#deleting the the extra data that doesnt fit the batch size\n",
    "modulus = len(new_train) %batch_size\n",
    "new_train_temp = new_train.iloc[ modulus: , :]\n",
    "new_train_temp = new_train_temp.reset_index()\n",
    "#print( new_train_temp )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014e54f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1ac7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### setting hyperparameters and gettings epoch sizes\n",
    "num_epochs = 2\n",
    "num_samples_train = len( new_train )\n",
    "num_batches_train = (num_samples_train // batch_size ) \n",
    "num_samples_valid = len( new_train )\n",
    "num_batches_valid = (num_samples_valid // batch_size ) \n",
    "\n",
    "# setting up lists for handling loss/accuracy\n",
    "train_acc, train_loss = [], []\n",
    "valid_acc, valid_loss = [], []\n",
    "test_acc, test_loss = [], []\n",
    "\n",
    "losses_train_train = []\n",
    "Losses_train_valid = []\n",
    "Losses_test = []\n",
    "#get_slice = lambda i, size: range(i * size, (i + 1) * size)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward -> Backprob -> Update params\n",
    "    ## Train\n",
    "    train_loader = iter(DataLoader(TimeSeriesDataSet(new_train_temp['Direction_50m'], new_train_temp['Speed_50m'], new_train_temp['Speed_50m'] ), batch_size= batch_size_own, shuffle=False))\n",
    "\n",
    "    train_train_cur_loss = 0\n",
    "    net.train()\n",
    "    print( f'new epoch, number{epoch}')\n",
    "    for i in range(num_batches_train-10):\n",
    "        #print( f'{i} run')\n",
    "        optimizer.zero_grad()\n",
    "        x, y, z = train_loader.next()\n",
    "        output = net( y.float() )\n",
    "        \n",
    "        \n",
    "        # compute gradients given loss\n",
    "        # get the target ==> Z\n",
    "        batch_loss = criterion(output.float(), z.float())\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        train_train_cur_loss += batch_loss   \n",
    "    losses_train_train.append(train_train_cur_loss / batch_size)\n",
    "    \n",
    "    net.eval()\n",
    "    ### Evaluate training\n",
    "    train_loader = iter(DataLoader(TimeSeriesDataSet(new_train_temp['Direction_50m'], new_train_temp['Speed_50m'], new_train_temp['Speed_50m'] ), batch_size= batch_size_own, shuffle=False))\n",
    "    train_val_cur_loss = 0\n",
    "    for i in range(num_batches_train):\n",
    "        x, y, z = train_loader.next()\n",
    "        output = net( y.float() )\n",
    "        \n",
    "        batch_loss = criterion(output.float(), z.float())\n",
    "        \n",
    "        train_val_cur_loss += batch_loss.detach().numpy()\n",
    "    Losses_train_valid.append(train_val_cur_loss / batch_size)\n",
    "    \n",
    "    ### Evaluate validation\n",
    "    test_loader = iter(DataLoader(TimeSeriesDataSet(new_train_temp['Direction_50m'], new_train_temp['Speed_50m'], new_train_temp['Speed_50m'] ), batch_size= batch_size_own, shuffle=False))\n",
    "    test_cur_loss = 0\n",
    "    for i in range(num_batches_valid):\n",
    "        x, y, z = test_loader.next()\n",
    "        output = net( y.float() )\n",
    "        \n",
    "        batch_loss = criterion(output.float(), z.float())\n",
    "        \n",
    "        test_cur_loss += batch_loss.detach().numpy()\n",
    "    Losses_test.append(test_cur_loss / batch_size)\n",
    "    \n",
    "    \n",
    "epoch = np.arange(len(Losses_test))\n",
    "plt.figure()\n",
    "plt.plot( Losses_train_valid, 'r', Losses_test, 'b') #, epoch, Losses_test, 'b')\n",
    "plt.legend(['Train Loss','Validation Loss'])\n",
    "plt.xlabel('Updates'), plt.ylabel('Acc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
