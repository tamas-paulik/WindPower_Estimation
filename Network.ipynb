{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff1c7b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Date_Time  Direction_10m  Speed_10m  Temperature_10m  \\\n",
      "0      2020-05-18 13:15:00       0.233983   0.523846         0.708869   \n",
      "1      2020-05-18 13:30:00       0.236769   0.527692         0.706422   \n",
      "2      2020-05-18 13:45:00       0.242340   0.530000         0.703976   \n",
      "3      2020-05-18 14:00:00       0.247911   0.530769         0.701835   \n",
      "4      2020-05-18 14:15:00       0.250696   0.530000         0.699847   \n",
      "...                    ...            ...        ...              ...   \n",
      "29436  2021-01-25 18:45:00       0.370474   0.176154         0.463456   \n",
      "29437  2021-01-25 19:00:00       0.370474   0.176154         0.463456   \n",
      "29438  2021-01-25 19:15:00       0.370474   0.176154         0.463456   \n",
      "29439  2021-01-25 19:30:00       0.370474   0.176154         0.463456   \n",
      "29440  2021-01-25 19:45:00       0.370474   0.176154         0.463456   \n",
      "\n",
      "       Pressure_seaLevel  Air_Density_10m  Direction_50m  Speed_50m  \\\n",
      "0               0.512525         0.250737       0.231198   0.547321   \n",
      "1               0.512159         0.253687       0.236769   0.550893   \n",
      "2               0.511977         0.253687       0.242340   0.552679   \n",
      "3               0.511977         0.256637       0.247911   0.553571   \n",
      "4               0.512159         0.259587       0.250696   0.552679   \n",
      "...                  ...              ...            ...        ...   \n",
      "29436           0.353812         0.427729       0.334262   0.151786   \n",
      "29437           0.353812         0.427729       0.334262   0.151786   \n",
      "29438           0.353812         0.427729       0.334262   0.151786   \n",
      "29439           0.353812         0.427729       0.334262   0.151786   \n",
      "29440           0.353812         0.427729       0.334262   0.151786   \n",
      "\n",
      "       Temperature_50m  Air_Density_50m  Direction_100m  Speed_100m  \\\n",
      "0             0.708869         0.250737        0.231198    0.469543   \n",
      "1             0.706422         0.253687        0.236769    0.473604   \n",
      "2             0.703976         0.253687        0.242340    0.476142   \n",
      "3             0.701835         0.256637        0.247911    0.477157   \n",
      "4             0.699847         0.259587        0.250696    0.476142   \n",
      "...                ...              ...             ...         ...   \n",
      "29436         0.463456         0.427729        0.426184    0.229442   \n",
      "29437         0.463456         0.427729        0.426184    0.229442   \n",
      "29438         0.463456         0.427729        0.426184    0.229442   \n",
      "29439         0.463456         0.427729        0.426184    0.229442   \n",
      "29440         0.463456         0.427729        0.426184    0.229442   \n",
      "\n",
      "       Temperature_100m  Air_Density_100m  Direction_150m  Speed_150m  \\\n",
      "0              0.708869          0.250737        0.233983    0.438298   \n",
      "1              0.706422          0.253687        0.236769    0.442908   \n",
      "2              0.703976          0.253687        0.242340    0.445745   \n",
      "3              0.701835          0.256637        0.247911    0.446809   \n",
      "4              0.699847          0.259587        0.250696    0.445745   \n",
      "...                 ...               ...             ...         ...   \n",
      "29436          0.463456          0.427729        0.445682    0.270922   \n",
      "29437          0.463456          0.427729        0.445682    0.270922   \n",
      "29438          0.463456          0.427729        0.445682    0.270922   \n",
      "29439          0.463456          0.427729        0.445682    0.270922   \n",
      "29440          0.463456          0.427729        0.445682    0.270922   \n",
      "\n",
      "       Temperature_150m  Air_Density_150m  Park_Power_[KW]  \n",
      "0              0.708869          0.250737         0.637287  \n",
      "1              0.706422          0.253687         0.623601  \n",
      "2              0.703976          0.253687         0.609736  \n",
      "3              0.701835          0.256637         0.579930  \n",
      "4              0.699847          0.259587         0.460988  \n",
      "...                 ...               ...              ...  \n",
      "29436          0.463456          0.427729         0.103521  \n",
      "29437          0.463456          0.427729         0.173047  \n",
      "29438          0.463456          0.427729         0.229187  \n",
      "29439          0.463456          0.427729         0.191322  \n",
      "29440          0.463456          0.427729         0.161436  \n",
      "\n",
      "[29441 rows x 19 columns]\n"
     ]
    }
   ],
   "source": [
    "#The datas are not uploaded to the repository, extract them to your base folder\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Cebrina's   Path: C:\\Users\\cebri\\Documents\\Wind Power Estimation\\Data\n",
    "# Guillermo's Path: C:\\DTU\\\\02456 - Deep Learning\\Project\\Datasets\n",
    "# Tomi's      Path: C:\\Users\\PC\\Documents\\GitHub\\WindPower_Estimation\n",
    "\n",
    "dataPath = r'C:\\Users\\PC\\Documents\\GitHub\\WindPower_Estimation'\n",
    "train1_dataset = pd.read_csv (r''+dataPath+'\\Case1\\Dataset_Train_1.csv', header=0, delim_whitespace=False)\n",
    "train2_dataset = pd.read_csv (r''+dataPath+'\\Case2\\Dataset_Train_2.csv', header=0, delim_whitespace=False)\n",
    "train3_dataset = pd.read_csv (r''+dataPath+'\\Case3\\Dataset_Train_3.csv', header=0, delim_whitespace=False)\n",
    "test1_dataset = pd.read_csv (r''+dataPath+'\\Case1\\Dataset_Test_1.csv', header=0, delim_whitespace=False)\n",
    "test2_dataset = pd.read_csv (r''+dataPath+'\\Case2\\Dataset_Test_2.csv', header=0, delim_whitespace=False)\n",
    "test3_dataset = pd.read_csv (r''+dataPath+'\\Case3\\Dataset_Test_3.csv', header=0, delim_whitespace=False)\n",
    "\n",
    "\n",
    "\n",
    "# ######################\n",
    "# batch_size = 16\n",
    "# #deleting the the extra data that doesnt fit the batch size\n",
    "# modulus = len(train1_dataset) %batch_size\n",
    "# train1_dataset = train1_dataset.iloc[ modulus: , :]\n",
    "# train1_dataset = train1_dataset.reset_index()\n",
    "# #print( new_train_temp )\n",
    "\n",
    "\n",
    "\n",
    "# test1_dataset = test1_dataset.drop(axis=1, columns= ['Date_Time', 'Direction_10m', 'Speed_10m', 'Temperature_10m', 'Pressure_seaLevel', 'Air_Density_10m', 'Temperature_50m', 'Air_Density_50m', 'Direction_100m', 'Speed_100m', 'Temperature_100m','Air_Density_100m', 'Direction_150m', 'Speed_150m', 'Temperature_150m', 'Air_Density_150m'] )     \n",
    "# test1_dataset = test1_dataset.to_numpy()\n",
    "# test1_dataset = torch.tensor( test1_dataset )           \n",
    "\n",
    "# train1_dataset = train1_dataset.drop(axis=1, columns= ['Date_Time', 'Direction_10m', 'Speed_10m', 'Temperature_10m', 'Pressure_seaLevel', 'Air_Density_10m', 'Temperature_50m', 'Air_Density_50m', 'Direction_100m', 'Speed_100m', 'Temperature_100m','Air_Density_100m', 'Direction_150m', 'Speed_150m', 'Temperature_150m', 'Air_Density_150m'] )     \n",
    "# train1_dataset = train1_dataset.to_numpy()\n",
    "# train1_dataset = torch.tensor( train1_dataset )           \n",
    "                                                     \n",
    "# temp = test1_dataset['Date_Time'].astype('datetime64',copy=False)\n",
    "# temp = torch.tensor(temp.values)\n",
    "# test1_dataset = torch.tensor(test1_dataset.values)\n",
    "# print( temp )\n",
    "\n",
    "\n",
    "#torch_tensor = torch.from_numpy( test1_dataset.values.astype(np.float32) )\n",
    "print( test1_dataset ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c91d7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79ddb04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#print(torch.__version__)\n",
    "#print(torch.version.cuda)\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load functions\n",
    "import torch.nn.init as init\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import Linear, GRU, Conv2d, Dropout, MaxPool2d, BatchNorm1d\n",
    "from torch.nn.functional import relu, elu, relu6, sigmoid, tanh, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efe41532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of class for batch generation\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class TimeSeriesDataSet(Dataset):\n",
    "  \"\"\"\n",
    "  This is a custom dataset class. It can get more complex than this, but simplified so you can understand what's happening here without\n",
    "  getting bogged down by the preprocessing\n",
    "  \"\"\"\n",
    "  def __init__(self, X, Y, Z):\n",
    "    self.X = X\n",
    "    self.Y = Y\n",
    "    self.Z = Z\n",
    "    if len(self.X) != len(self.Y) :\n",
    "      raise Exception(\"The length of X does not match the length of Y\")\n",
    "    if len(self.X) != len(self.Z) :\n",
    "      raise Exception(\"The length of X does not match the length of Z\")\n",
    "    if len(self.Z) != len(self.Y) :\n",
    "      raise Exception(\"The length of Z does not match the length of Y\")\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    # note that this isn't randomly selecting. It's a simple get a single item that represents an x and y\n",
    "    _x = self.X[index]\n",
    "    _y = self.Y[index]\n",
    "    _z = self.Z[index]\n",
    "\n",
    "    return _x , _y , _z "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6220213f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the FFNN architecture\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=1, learn_bias=False ):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.ffn1 = nn.Linear(in_features =in_channels,\n",
    "                              out_features =out_channels,\n",
    "                              bias = learn_bias)\n",
    "        \n",
    "        self.ffn2 = nn.Linear(in_features =mid_channels,\n",
    "                              out_features =out_channels,\n",
    "                              bias = learn_bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x= ( self.ffn1(x))\n",
    "        return x  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e741f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### not implemented so far=========\n",
    "def accuracy(ys, ts):\n",
    "    predictions = torch.max(ys, 1)[1]\n",
    "    correct_prediction = torch.eq(predictions, ts)\n",
    "    return torch.mean(correct_prediction.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0508da59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GPU.\n"
     ]
    }
   ],
   "source": [
    "# Putting to CUDA the dataset\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"Running GPU.\") if use_cuda else print(\"No GPU available.\")\n",
    "\n",
    "def get_variable(x):\n",
    "    \"\"\" Converts tensors to cuda, if available. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cuda()\n",
    "    return x\n",
    "\n",
    "def get_numpy(x):\n",
    "    \"\"\" Get numpy array for both cuda and not. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cpu().data.numpy()\n",
    "    return x.data.numpy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "861fd77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network creation and paramter modification\n",
    "\n",
    "batch_size = 16\n",
    "in_channels_setting = batch_size\n",
    "out_channels_setting = batch_size\n",
    "\n",
    "\n",
    "\n",
    "net = Net(in_channels_setting, out_channels_setting )\n",
    "# if use_cuda:\n",
    "#     net.cuda()\n",
    "#     train1_dataset.cuda\n",
    "#     test1_dataset.cuda\n",
    "#     print('run on cuda')\n",
    "# print(net)\n",
    "\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "criterion =  nn.MSELoss()       \n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "014e54f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc1ac7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new epoch, number0\n",
      "new epoch, number1\n"
     ]
    }
   ],
   "source": [
    "###### setting hyperparameters and gettings epoch sizes\n",
    "num_epochs = 2\n",
    "num_samples_train = len( train1_dataset )\n",
    "num_batches_train = (num_samples_train // batch_size ) \n",
    "num_samples_valid = len( test1_dataset )\n",
    "num_batches_valid = (num_samples_valid // batch_size ) \n",
    "\n",
    "# setting up lists for handling loss/accuracy\n",
    "train_acc, train_loss = [], []\n",
    "valid_acc, valid_loss = [], []\n",
    "test_acc, test_loss = [], []\n",
    "\n",
    "losses_train_train = []\n",
    "Losses_train_valid = []\n",
    "Losses_test = []\n",
    "#get_slice = lambda i, size: range(i * size, (i + 1) * size)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward -> Backprob -> Update params\n",
    "    ## Train\n",
    "    train_loader = iter(DataLoader(TimeSeriesDataSet(train1_dataset['Direction_50m'], train1_dataset['Speed_50m'], train1_dataset['Speed_50m'] ), batch_size= batch_size, shuffle=False))\n",
    "\n",
    "    train_train_cur_loss = 0\n",
    "    net.train()\n",
    "    print( f'new epoch, number{epoch}')\n",
    "    for i in range(num_batches_train-10):\n",
    "        #print( f'{i} run')\n",
    "        optimizer.zero_grad()\n",
    "        x, y, z = train_loader.next()\n",
    "        output = net( y.float() )\n",
    "        \n",
    "        \n",
    "        # compute gradients given loss\n",
    "        # get the target ==> Z\n",
    "        batch_loss = criterion(output.float(), z.float())\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        train_train_cur_loss += batch_loss   \n",
    "    losses_train_train.append(train_train_cur_loss / batch_size)\n",
    "    \n",
    "    net.eval()\n",
    "    ### Evaluate training\n",
    "    train_loader = iter(DataLoader(TimeSeriesDataSet(train1_dataset['Direction_50m'], train1_dataset['Speed_50m'], train1_dataset['Speed_50m'] ), batch_size= batch_size, shuffle=False))\n",
    "    train_val_cur_loss = 0\n",
    "    for i in range(num_batches_train):\n",
    "        x, y, z = train_loader.next()\n",
    "        output = net( y.float() )\n",
    "        \n",
    "        batch_loss = criterion(output.float(), z.float())\n",
    "        \n",
    "        train_val_cur_loss += batch_loss.detach().numpy()\n",
    "    Losses_train_valid.append(train_val_cur_loss / batch_size)\n",
    "    \n",
    "    ### Evaluate validation\n",
    "    test_loader = iter(DataLoader(TimeSeriesDataSet(test1_dataset['Direction_50m'], test1_dataset['Speed_50m'], test1_dataset['Speed_50m'] ), batch_size= batch_size, shuffle=False))\n",
    "    test_cur_loss = 0\n",
    "    for i in range(num_batches_valid):\n",
    "        x, y, z = test_loader.next()\n",
    "        output = net( y.float() )\n",
    "        \n",
    "        batch_loss = criterion(output.float(), z.float())\n",
    "        \n",
    "        test_cur_loss += batch_loss.detach().numpy()\n",
    "    Losses_test.append(test_cur_loss / batch_size)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ac88116",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6ab6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot( Losses_train_valid, 'r', Losses_test, 'b') #, epoch, Losses_test, 'b')\n",
    "plt.legend(['Train Loss','Validation Loss'])\n",
    "plt.xlabel('Updates'), plt.ylabel('Acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dd76b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "y = 2 * 10 + 5 \n",
    "plt.title(\"Matplotlib demo\") \n",
    "plt.xlabel(\"x axis caption\") \n",
    "plt.ylabel(\"y axis caption\") \n",
    "plt.plot(y) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ea56aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8a1e84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
